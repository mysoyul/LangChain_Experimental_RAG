{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1F5lTDp5UPf0",
   "metadata": {
    "id": "1F5lTDp5UPf0"
   },
   "source": [
    "### 1) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-openai langchain_community chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152049-e9e5-4952-8e19-409f58cf3ac9",
   "metadata": {
    "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
   },
   "source": [
    "### 2) OpenAI ì¸ì¦í‚¤ ì„¤ì •\n",
    "https://openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {
    "id": "b76f68a8-4745-4377-8057-6090b87377d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9abc5",
   "metadata": {},
   "source": [
    "##### Chroma ê°„ë‹¨í•œ ì˜ˆì œ\n",
    "* Chroma DBì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings  # OpenAI ì„ë² ë”© ì‚¬ìš©\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "from langchain_chroma import Chroma  # ë²¡í„° DB (Chroma) ì‚¬ìš©\n",
    "\n",
    "\n",
    "# 2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "DB_PATH = \"./db/chroma_db\"\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def load_and_split_text(file_path, splitter):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•œ í›„, ì„¤ì •ëœ Splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n",
    "        splitter (RecursiveCharacterTextSplitter): í…ìŠ¤íŠ¸ ë¶„í• ê¸° ê°ì²´\n",
    "\n",
    "    Returns:\n",
    "        list: ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        loader = TextLoader(file_path)  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\n",
    "        return loader.load_and_split(splitter)  # ë¶„í• í•˜ì—¬ ë°˜í™˜\n",
    "    except Exception as e:\n",
    "        print(f\" íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# 4. í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (600ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , 100ì ê²¹ì¹¨ í¬í•¨)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "# 5. ë‘ ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ë° ë¶„í• \n",
    "split_doc1 = load_and_split_text(\"data/ai-terminology.txt\", text_splitter)\n",
    "split_doc2 = load_and_split_text(\"data/finance-terminology.txt\", text_splitter)\n",
    "\n",
    "# 6. ë¬¸ì„œ ê°œìˆ˜ ì¶œë ¥\n",
    "print(f\"AI ë¬¸ì„œ ê°œìˆ˜: {len(split_doc1)}\")\n",
    "print(f\"ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: {len(split_doc2)}\")\n",
    "\n",
    "# 7. ëª¨ë“  ë¬¸ì„œ í•©ì¹˜ê¸°\n",
    "all_documents = split_doc1 + split_doc2\n",
    "\n",
    "# 8. Chroma ë²¡í„° DB ìƒì„± ë° ì €ì¥\n",
    "try:\n",
    "    persist_db = Chroma.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=OpenAIEmbeddings(),  # OpenAI Embeddings ì‚¬ìš©\n",
    "        persist_directory=DB_PATH,  # ë²¡í„° DB ì €ì¥ ìœ„ì¹˜ ì§€ì •\n",
    "        collection_name=\"my_vector_db\",  # ë°ì´í„°ë² ì´ìŠ¤ ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "    )\n",
    "    print(\"Chroma ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\" Chroma ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 9. ì €ì¥ëœ ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    retrieved_docs = persist_db.get()  # Chroma DBì—ì„œ ë°ì´í„° ì¡°íšŒ\n",
    "    print(f\" ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {len(retrieved_docs['ids'])}, íƒ€ì… {type(retrieved_docs['ids'])}\")\n",
    "except Exception as e:\n",
    "    print(f\" ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 10. ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_query(query, k=2):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì…ë ¥(query)ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰í•  ë¬¸ì¥ (ì˜ˆ: \"Transformer ê°œë… ì„¤ëª…\")\n",
    "        k (int, optional): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        None: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search(query, k=k)  # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"ğŸ”¹ [Result {i+1}]: {doc.page_content[:300]}...\\n\")  # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "    except Exception as e:\n",
    "        print(f\" ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 11. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "search_query(\"Transformer ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=2)\n",
    "search_query(\"Hedge Fund ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜?\", k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218ba88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI ë¬¸ì„œ ê°œìˆ˜: 6\n",
      "ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: 5\n",
      "FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\n",
      " ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: 11\n",
      " ë²¡í„° ì°¨ì›: 1536\n",
      " ì¸ë±ìŠ¤ íƒ€ì…: <class 'faiss.swigfaiss_avx2.IndexFlatL2'>\n",
      "\n",
      " [Query]: Transformer ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\n",
      "\n",
      "[Result 1]: Token (í† í°)\n",
      "\n",
      "ì •ì˜: í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë” ì‘ì€ ë‹¨ìœ„(ë‹¨ì–´, ë¬¸ì, ë¬¸ì¥ ë“±)ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •.\n",
      "ì˜ˆì‹œ: \"AIëŠ” í˜ì‹ ì ì´ë‹¤\"ë¥¼ [\"AI\", \"ëŠ”\", \"í˜ì‹ ì \", \"ì´ë‹¤\"]ë¡œ ë¶„í• .\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: í† í°í™”, NLP, í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
      "\n",
      "Transformer (íŠ¸ëœìŠ¤í¬ë¨¸)\n",
      "\n",
      "ì •ì˜: ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ë¡œ, ë³‘ë ¬ ì—°ì‚°ê³¼ ì¥ê¸° ì˜ì¡´ì„± ì²˜ë¦¬ê°€ ê°•ì .\n",
      "ì˜ˆì‹œ: GPT, BERT ë“±ì˜ ëª¨ë¸ì´ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ë”¥ëŸ¬ë‹, ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜, NLP\n",
      "\n",
      "Self-Attention (ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜)\n",
      "\n",
      "ì •ì˜...\n",
      "\n",
      "[Result 2]: Earnings Per Share (ì£¼ë‹¹ìˆœì´ìµ, EPS)\n",
      "\n",
      "ì •ì˜: ê¸°ì—…ì´ ë²Œì–´ë“¤ì¸ ìˆœì´ìµì„ ì´ ë°œí–‰ ì£¼ì‹ ìˆ˜ë¡œ ë‚˜ëˆˆ ê°’.\n",
      "ì˜ˆì‹œ: ì• í”Œì˜ EPS ì¦ê°€ë¡œ ì¸í•´ ì£¼ê°€ ìƒìŠ¹ ê¸°ëŒ€ê°ì´ ì»¤ì§.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ê¸°ì—… ì‹¤ì , íˆ¬ì íŒë‹¨, ì£¼ì‹ ë¶„ì„\n",
      "\n",
      "Interest Rate (ì´ììœ¨)\n",
      "\n",
      "ì •ì˜: ëˆì„ ë¹Œë¦´ ë•Œ ì§€ë¶ˆí•´ì•¼ í•˜ëŠ” ë¹„ìš©ì˜ ë¹„ìœ¨.\n",
      "ì˜ˆì‹œ: ì—°ë°©ì¤€ë¹„ì œë„(Fed)ê°€ ê¸°ì¤€ê¸ˆë¦¬ë¥¼ ì¸ìƒí•˜ë©´ ëŒ€ì¶œ ì´ììœ¨ë„ ìƒìŠ¹í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì¤‘ì•™ì€í–‰, ì±„ê¶Œ ì‹œì¥, ê²½ì œ ì •ì±…\n",
      "\n",
      "Inflation (ì¸í”Œë ˆì´ì…˜)\n",
      "\n",
      "ì •ì˜: ë¬¼ê°€ê°€ ì „ë°˜ì ìœ¼ë¡œ ì§€ì†í•´ì„œ ìƒìŠ¹í•˜ëŠ” ê²½ì œ í˜„ìƒ.\n",
      "ì˜ˆ...\n",
      "\n",
      "\n",
      " [Query]: Hedge Fund ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜?\n",
      "\n",
      "[Result 1]: Mutual Fund (ë®¤ì¶”ì–¼ í€ë“œ)\n",
      "\n",
      "ì •ì˜: ì—¬ëŸ¬ íˆ¬ììì˜ ìê¸ˆì„ ëª¨ì•„ ë‹¤ì–‘í•œ ìì‚°ì— íˆ¬ìí•˜ëŠ” í€ë“œ.\n",
      "ì˜ˆì‹œ: ë®¤ì¶”ì–¼ í€ë“œëŠ” ë¶„ì‚° íˆ¬ìë¡œ ìœ„í—˜ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì„ ì¤Œ.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ê°„ì ‘ íˆ¬ì, í¬íŠ¸í´ë¦¬ì˜¤, í€ë“œ ë§¤ë‹ˆì €\n",
      "\n",
      "Hedge Fund (í—¤ì§€í€ë“œ)\n",
      "\n",
      "ì •ì˜: ê³µê²©ì ì¸ íˆ¬ì ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ë†’ì€ ìˆ˜ìµì„ ì¶”êµ¬í•˜ëŠ” íˆ¬ì í€ë“œ.\n",
      "ì˜ˆì‹œ: í—¤ì§€í€ë“œëŠ” ê³µë§¤ë„, ë ˆë²„ë¦¬ì§€ ë“± ë‹¤ì–‘í•œ ì „ëµì„ í™œìš©í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ê³ ìœ„í—˜ íˆ¬ì, ì ê·¹ì  ìš´ìš©, ë ˆë²„ë¦¬ì§€\n",
      "\n",
      "Asset Allocation (ìì‚° ë°°ë¶„)\n",
      "\n",
      "ì •ì˜: íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ì—ì„œ ì£¼ì‹, ì±„ê¶Œ, í˜„ê¸ˆ ...\n",
      "\n",
      "[Result 2]: Federal Reserve (ì—°ë°©ì¤€ë¹„ì œë„, Fed)\n",
      "\n",
      "ì •ì˜: ë¯¸êµ­ì˜ ì¤‘ì•™ì€í–‰ìœ¼ë¡œ, ê¸ˆë¦¬ ì •ì±…ê³¼ í†µí™” ê³µê¸‰ì„ ì¡°ì ˆí•¨.\n",
      "ì˜ˆì‹œ: Fedê°€ ê¸ˆë¦¬ë¥¼ ì¸ìƒí•˜ë©´ ì‹œì¥ ìœ ë™ì„±ì´ ê°ì†Œí•  ìˆ˜ ìˆìŒ.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ê¸ˆë¦¬, í†µí™”ì •ì±…, ê²½ì œ ì¡°ì •\n",
      "\n",
      "Bond (ì±„ê¶Œ)\n",
      "\n",
      "ì •ì˜: ì •ë¶€ë‚˜ ê¸°ì—…ì´ ìê¸ˆì„ ì¡°ë‹¬í•˜ê¸° ìœ„í•´ ë°œí–‰í•˜ëŠ” ë¶€ì±„ ì¦ì„œë¡œ, ì¼ì • ê¸°ê°„ ë™ì•ˆ ì´ìë¥¼ ì§€ê¸‰í•˜ê³  ë§Œê¸°ì— ì›ê¸ˆì„ ìƒí™˜í•¨.\n",
      "ì˜ˆì‹œ: ë¯¸êµ­ êµ­ì±„ëŠ” ì•ˆì „í•œ íˆ¬ì ìˆ˜ë‹¨ìœ¼ë¡œ ê°„ì£¼ë¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ê³ ì • ìˆ˜ìµ, íˆ¬ì, ê¸ˆë¦¬\n",
      "\n",
      "Stock (ì£¼ì‹)\n",
      "\n",
      "ì •ì˜: ê¸°ì—…ì˜ ì§€ë¶„ì„ ë‚˜íƒ€ë‚´ë©°, ì£¼ì‹ì„ ë³´ìœ í•œ íˆ¬ììëŠ” ...\n",
      "\n",
      "\n",
      "==================================================\n",
      "ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
      "==================================================\n",
      "FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "ë¡œë“œëœ ë²¡í„° ê°œìˆ˜: 11\n",
      "\n",
      "ë¡œë“œëœ DB ê²€ìƒ‰ ê²°ê³¼: Explainable AI (ì„¤ëª… ê°€ëŠ¥í•œ AI, XAI)\n",
      "\n",
      "ì •ì˜: AIì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì´ ì‚¬ëŒì—ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…ë˜ëŠ” ê¸°ìˆ .\n",
      "ì˜ˆì‹œ: AIê°€ ì˜ë£Œ ì§„ë‹¨ì„ ë‚´ë¦´ ë•Œ, ì™œ ê·¸ëŸ° ê²°ë¡ ì„ ë‚´ë ¸ëŠ”ì§€ ì„¤ëª…í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì‹ ë¢°ì„±, ëª¨ë¸ í•´ì„, AI ìœ¤ë¦¬\n",
      "\n",
      "AGI (Artificial General Intelligence, ë²”ìš© ì¸ê³µì§€ëŠ¥)\n",
      "\n",
      "ì •ì˜: íŠ¹ì • ì‘ì—…ì´ ...\n",
      "\n",
      "==================================================\n",
      "ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\n",
      "==================================================\n",
      "\n",
      " [Query]: ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹\n",
      "\n",
      "[Result 1] (Score: 0.3324):\n",
      "Zero-shot Learning (ì œë¡œìƒ· í•™ìŠµ)\n",
      "\n",
      "ì •ì˜: íŠ¹ì • íƒœìŠ¤í¬ì— ëŒ€í•œ ë°ì´í„° ì—†ì´ë„ ê¸°ì¡´ í•™ìŠµëœ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì˜ˆì¸¡í•˜ëŠ” AI ê¸°ìˆ .\n",
      "ì˜ˆì‹œ: AIê°€ ìƒˆë¡œìš´ ì–¸ì–´ë¥¼ í•™ìŠµí•˜ì§€ ì•Šì•˜ì§€ë§Œ, ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë²ˆì—­ ê°€ëŠ¥.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì „ì´ í•™ìŠµ, ì¼ë°˜í™” ëŠ¥ë ¥, NLP\n",
      "\n",
      "Few-shot Learning (í“¨ìƒ· í•™ìŠµ)\n",
      "\n",
      "ì •ì˜: ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ìƒˆë¡œìš´ ì‘ì—…...\n",
      "\n",
      "[Result 2] (Score: 0.3566):\n",
      "Diffusion Model (í™•ì‚° ëª¨ë¸)\n",
      "\n",
      "ì •ì˜: ì´ë¯¸ì§€ ìƒì„± AIì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì ì§„ì ìœ¼ë¡œ ì œê±°í•˜ë©° ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ë²•.\n",
      "ì˜ˆì‹œ: Stable Diffusionì´ í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì´ë¯¸ì§€ ìƒì„±, ë”¥ëŸ¬ë‹, ìƒì„± AI\n",
      "\n",
      "Hallucination (í™˜ê°)\n",
      "\n",
      "ì •ì˜: AIê°€ ì‹¤ì œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ë‚˜ ì˜ëª»ëœ ë‚´ìš©ì„ ìƒì„±í•˜ëŠ” í˜„ìƒ....\n",
      "\n",
      "[Result 3] (Score: 0.3712):\n",
      "Large Language Model (ëŒ€í˜• ì–¸ì–´ ëª¨ë¸, LLM)\n",
      "\n",
      "ì •ì˜: ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•œ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸.\n",
      "ì˜ˆì‹œ: GPT-4, PaLM ë“±ì´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì— í•´ë‹¹.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ìì—°ì–´ ì²˜ë¦¬, íŠ¸ëœìŠ¤í¬ë¨¸, AI ëª¨ë¸\n",
      "\n",
      "Vector Database (ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤)\n",
      "\n",
      "ì •ì˜: ë°ì´í„°ë¥¼ ë²¡í„° í˜•íƒœë¡œ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ë°ì´...\n",
      "\n",
      "\n",
      "==================================================\n",
      "FAISS ì¸ë±ìŠ¤ ìƒì„¸ ì •ë³´\n",
      "==================================================\n",
      "ì¸ë±ìŠ¤ íƒ€ì…: IndexFlatL2\n",
      "ì´ ë²¡í„° ìˆ˜: 11\n",
      "ë²¡í„° ì°¨ì›: 1536\n",
      "í›ˆë ¨ ì—¬ë¶€: True\n",
      "ë©”íŠ¸ë¦­ íƒ€ì…: 1\n",
      "ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.06 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings  # OpenAI ì„ë² ë”© ì‚¬ìš©\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "from langchain_community.vectorstores import FAISS  # ë²¡í„° DB (FAISS) ì‚¬ìš©\n",
    "\n",
    "# 2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "DB_PATH = \"./db/faiss_db\"\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def load_and_split_text(file_path, splitter):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•œ í›„, ì„¤ì •ëœ Splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n",
    "        splitter (RecursiveCharacterTextSplitter): í…ìŠ¤íŠ¸ ë¶„í• ê¸° ê°ì²´\n",
    "\n",
    "    Returns:\n",
    "        list: ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\n",
    "        return loader.load_and_split(splitter)  # ë¶„í• í•˜ì—¬ ë°˜í™˜\n",
    "    except Exception as e:\n",
    "        print(f\" íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# 4. í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (600ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , 100ì ê²¹ì¹¨ í¬í•¨)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "# 5. ë‘ ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ë° ë¶„í• \n",
    "split_doc1 = load_and_split_text(\"data/ai-terminology.txt\", text_splitter)\n",
    "split_doc2 = load_and_split_text(\"data/finance-terminology.txt\", text_splitter)\n",
    "\n",
    "# 6. ë¬¸ì„œ ê°œìˆ˜ ì¶œë ¥\n",
    "print(f\"AI ë¬¸ì„œ ê°œìˆ˜: {len(split_doc1)}\")\n",
    "print(f\"ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: {len(split_doc2)}\")\n",
    "\n",
    "# 7. ëª¨ë“  ë¬¸ì„œ í•©ì¹˜ê¸°\n",
    "all_documents = split_doc1 + split_doc2\n",
    "\n",
    "# 8. FAISS ë²¡í„° DB ìƒì„± ë° ì €ì¥\n",
    "try:\n",
    "    # FAISS ë²¡í„° DB ìƒì„±\n",
    "    persist_db = FAISS.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=OpenAIEmbeddings(),  # OpenAI Embeddings ì‚¬ìš©\n",
    "    )\n",
    "    \n",
    "    # ë¡œì»¬ ë””ìŠ¤í¬ì— ì €ì¥\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        os.makedirs(DB_PATH)\n",
    "    persist_db.save_local(DB_PATH)\n",
    "    \n",
    "    print(\"FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\" FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 9. ì €ì¥ëœ ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    # FAISSì—ì„œ ì¸ë±ìŠ¤ ì •ë³´ í™•ì¸\n",
    "    print(f\" ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {persist_db.index.ntotal}\")\n",
    "    print(f\" ë²¡í„° ì°¨ì›: {persist_db.index.d}\")\n",
    "    print(f\" ì¸ë±ìŠ¤ íƒ€ì…: {type(persist_db.index)}\")\n",
    "except Exception as e:\n",
    "    print(f\" ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 10. ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_query(query, k=2):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì…ë ¥(query)ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰í•  ë¬¸ì¥ (ì˜ˆ: \"Transformer ê°œë… ì„¤ëª…\")\n",
    "        k (int, optional): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        None: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search(query, k=k)  # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"[Result {i+1}]: {doc.page_content[:300]}...\\n\")  # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "    except Exception as e:\n",
    "        print(f\" ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 11. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "search_query(\"Transformer ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=2)\n",
    "search_query(\"Hedge Fund ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜?\", k=2)\n",
    "\n",
    "# 12. ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # ì €ì¥ëœ FAISS DB ë¡œë“œ\n",
    "    loaded_db = FAISS.load_local(\n",
    "        DB_PATH, \n",
    "        OpenAIEmbeddings(),\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(\"FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ë¡œë“œëœ DBë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"ë¡œë“œëœ ë²¡í„° ê°œìˆ˜: {loaded_db.index.ntotal}\")\n",
    "    \n",
    "    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "    test_results = loaded_db.similarity_search(\"ì¸ê³µì§€ëŠ¥ì´ë€?\", k=1)\n",
    "    print(f\"\\në¡œë“œëœ DB ê²€ìƒ‰ ê²°ê³¼: {test_results[0].page_content[:200]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FAISS DB ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 13. ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def search_with_score(query, k=2):\n",
    "    \"\"\"\n",
    "    ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search_with_score(query, k=k)\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, (doc, score) in enumerate(results):\n",
    "            print(f\"[Result {i+1}] (Score: {score:.4f}):\")\n",
    "            print(f\"{doc.page_content[:200]}...\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\" ì ìˆ˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "search_with_score(\"ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹\", k=3)\n",
    "\n",
    "# 14. FAISS ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FAISS ì¸ë±ìŠ¤ ìƒì„¸ ì •ë³´\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    index = persist_db.index\n",
    "    print(f\"ì¸ë±ìŠ¤ íƒ€ì…: {type(index).__name__}\")\n",
    "    print(f\"ì´ ë²¡í„° ìˆ˜: {index.ntotal}\")\n",
    "    print(f\"ë²¡í„° ì°¨ì›: {index.d}\")\n",
    "    print(f\"í›ˆë ¨ ì—¬ë¶€: {index.is_trained}\")\n",
    "    print(f\"ë©”íŠ¸ë¦­ íƒ€ì…: {index.metric_type}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì¶”ì •)\n",
    "    memory_usage = index.ntotal * index.d * 4 / (1024 * 1024)  # 4 bytes per float, MB ë‹¨ìœ„\n",
    "    print(f\"ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d910ef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chatbot-0lCeHk3W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
