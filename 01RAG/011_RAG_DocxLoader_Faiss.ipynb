{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -q docx2txt faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Knowledge Base 구성을 위한 데이터 생성\n",
    "\n",
    "- [RecursiveCharacterTextSplitter](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/)를 활용한 데이터 chunking\n",
    "    - split 된 데이터 chunk를 Large Language Model(LLM)에게 전달하면 토큰 절약 가능\n",
    "    - 비용 감소와 답변 생성시간 감소의 효과\n",
    "    - LangChain에서 다양한 [TextSplitter](https://python.langchain.com/v0.2/docs/how_to/#text-splitters)들을 제공\n",
    "- `chunk_size` 는 split 된 chunk의 최대 크기\n",
    "- `chunk_overlap`은 앞 뒤로 나뉘어진 chunk들이 얼마나 겹쳐도 되는지 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "loader = Docx2txtLoader('data/tax_with_table.docx')\n",
    "document_list = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "print(len(document_list))\n",
    "print(type(document_list[0]))\n",
    "print(document_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI에서 제공하는 Embedding Model을 활용해서 `chunk`를 vector화\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 데이터를 처음 저장할 때 \n",
    "database = FAISS.from_documents(documents=document_list, embedding=embedding)\n",
    "\n",
    "# 이미 저장된 데이터를 사용할 때 \n",
    "# database.save_local(\"./db/faiss\")\n",
    "# database = FAISS.load_local(\"./db/faiss\", embedding, allow_dangerous_deserialization=True)\n",
    "print(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 답변 생성을 위한 Retrieval\n",
    "\n",
    "- `FAISS`에 저장한 데이터를 유사도 검색(`similarity_search()`)를 활용해서 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '연봉 5천만원인 직장인의 소득세는 얼마인가요?'\n",
    "\n",
    "# `k` 값을 조절해서 얼마나 많은 데이터를 불러올지 결정\n",
    "retrieved_docs = database.similarity_search(query, k=3)\n",
    "\n",
    "print(len(retrieved_docs))\n",
    "print(type(retrieved_docs[0]))\n",
    "print(retrieved_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Augmentation을 위한 Prompt 활용\n",
    "\n",
    "- Retrieval된 데이터는 LangChain에서 제공하는 프롬프트(`\"rlm/rag-prompt\"`) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 답변 생성\n",
    "\n",
    "- [RetrievalQA](https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain)를 통해 LLM에 전달\n",
    "    - `RetrievalQA`는 [create_retrieval_chain](https://python.langchain.com/v0.2/docs/how_to/qa_sources/#using-create_retrieval_chain)으로 대체됨\n",
    "    - 실제 ChatBot 구현 시 `create_retrieval_chain`으로 변경하는 과정을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, \n",
    "    retriever=database.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '연봉 5천만원인 직장인의 소득세는 얼마인가요?'\n",
    "ai_message = qa_chain.invoke({\"query\": query})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '비과세소득에 어떤 것들이 있나요?'\n",
    "\n",
    "ai_message = qa_chain.invoke({\"query\": query})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LangChain 기반의 RAG(Retrieval-Augmented Generation) 파이프라인을 구현하여 DOCX 문서를 로드, 임베딩, 검색, 그리고 LLM을 통한 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 로드 완료\n",
      "문서 분할 완료: 720개 청크 생성\n",
      "벡터 저장소 생성 완료\n",
      "{'query': '총수입금액 불산입에 대하여 설명해 주세요.', 'result': '「국세기본법」 및 「지방세기본법」 등에 따르면, 총수입금액에는 개별소비세, 주세, 부가가치세의 매출세액, 및 특정 조합에 따라 환급받은 세액 및 거래 등에서 발생한 수입금액이 산입되지 않는다. 총수입금액의 계산과 관련된 범위와 규정은 대통령령에 따라 결정된다. 다만, 소득금액 계산에 있어 부담하는 세액은 총수입금액에 산입되지 않는다.'}\n",
      "\n",
      " AI의 답변:\n",
      "「국세기본법」 및 「지방세기본법」 등에 따르면, 총수입금액에는 개별소비세, 주세, 부가가치세의 매출세액, 및 특정 조합에 따라 환급받은 세액 및 거래 등에서 발생한 수입금액이 산입되지 않는다. 총수입금액의 계산과 관련된 범위와 규정은 대통령령에 따라 결정된다. 다만, 소득금액 계산에 있어 부담하는 세액은 총수입금액에 산입되지 않는다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # 특정 경고 유형만 무시\n",
    "\n",
    "#  1. 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API 키 확인\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OpenAI API 키가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "\n",
    "#  3. DOCX 파일 로드 및 텍스트 추출 (Docx2txtLoader 활용)\n",
    "def load_docx(file_path):\n",
    "    \"\"\"DOCX 파일에서 텍스트를 추출하는 함수.\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "#  4. 문서 분할 함수\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"텍스트를 지정된 크기의 청크로 분할하는 함수.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "#  5. 벡터 데이터베이스(FAISS) 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "    \"\"\"텍스트 청크를 임베딩하고 FAISS 벡터 저장소에 저장.\"\"\"\n",
    "    try:\n",
    "        documents = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "#  6. LLM을 활용한 질문 응답 함수\n",
    "def query_with_llm(query, vector_store):\n",
    "    \"\"\"LLM을 사용하여 검색된 문서 기반으로 답변 생성.\"\"\"\n",
    "    try:\n",
    "        # LLM 모델 설정\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "        \n",
    "        # 프롬프트 로드 (RAG 최적화된 LangChain Hub 프롬프트 사용)\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "        # RetrievalQA 체인 생성\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm, \n",
    "            retriever=vector_store.as_retriever(),\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "\n",
    "        # LLM 응답 생성\n",
    "        ai_message = qa_chain.invoke({\"query\": query})\n",
    "        print(ai_message)\n",
    "        return ai_message[\"result\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "#  실행 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # DOCX 파일 경로\n",
    "    docx_path = \"data/tax_with_table.docx\"\n",
    "    \n",
    "    # 1. 문서 로드\n",
    "    text = load_docx(docx_path)\n",
    "    print(\"문서 로드 완료\")\n",
    "    \n",
    "    # 2. 문서 분할\n",
    "    text_chunks = split_text(text)\n",
    "    print(f\"문서 분할 완료: {len(text_chunks)}개 청크 생성\")\n",
    "    \n",
    "    # 3. 임베딩 모델 초기화\n",
    "    embedding_model = OpenAIEmbeddings()\n",
    "    \n",
    "    # 4. 벡터 저장소 생성\n",
    "    vector_store = create_vector_store(text_chunks, embedding_model)\n",
    "    print(\"벡터 저장소 생성 완료\")\n",
    "    \n",
    "    # 5. 질의 실행\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store)\n",
    "    \n",
    "    # 6. AI 응답 출력\n",
    "    print(\"\\n AI의 답변:\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  개선된 Source\n",
    "```\n",
    "RuntimeError: 벡터 저장소 생성 실패: Error code: 400 - \n",
    "{'error': {\n",
    "    'message': 'Requested 313741 tokens, max 300000 tokens per request', \n",
    "    'type': 'max_tokens_per_request', \n",
    "    'param': None, 'code': 'max_tokens_per_request'\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 개선된 RAG 파이프라인 실행 ===\n",
      "\n",
      "1. 문서 로드 중...\n",
      "   문서 로드 완료: 289,214 문자\n",
      "\n",
      "2. 문서 분할 중...\n",
      "   문서 분할 완료: 511개 청크 생성\n",
      "\n",
      "3. 임베딩 모델 초기화...\n",
      "   임베딩 모델 초기화 완료\n",
      "\n",
      "4. 벡터 저장소 생성 중...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "벡터 저장소 생성 실패: Error code: 400 - {'error': {'message': 'Requested 313741 tokens, max 300000 tokens per request', 'type': 'max_tokens_per_request', 'param': None, 'code': 'max_tokens_per_request'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mcreate_vector_store\u001b[39m\u001b[34m(text_chunks, embedding_model)\u001b[39m\n\u001b[32m    102\u001b[39m     documents.append(Document(page_content=chunk, metadata=metadata))\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m vector_store = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vector_store, documents\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vega2\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chatbot-0lCeHk3W-py3.12\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:848\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    846\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vega2\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chatbot-0lCeHk3W-py3.12\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m     texts,\n\u001b[32m   1046\u001b[39m     embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m     **kwargs,\n\u001b[32m   1051\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vega2\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chatbot-0lCeHk3W-py3.12\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:590\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    589\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vega2\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chatbot-0lCeHk3W-py3.12\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:478\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vega2\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chatbot-0lCeHk3W-py3.12\\Lib\\site-packages\\openai\\resources\\embeddings.py:128\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vega2\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chatbot-0lCeHk3W-py3.12\\Lib\\site-packages\\openai\\_base_client.py:1276\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1273\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1274\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1275\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vega2\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chatbot-0lCeHk3W-py3.12\\Lib\\site-packages\\openai\\_base_client.py:949\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    947\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vega2\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chatbot-0lCeHk3W-py3.12\\Lib\\site-packages\\openai\\_base_client.py:1057\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1056\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1060\u001b[39m     cast_to=cast_to,\n\u001b[32m   1061\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1065\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1066\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'Requested 313741 tokens, max 300000 tokens per request', 'type': 'max_tokens_per_request', 'param': None, 'code': 'max_tokens_per_request'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 291\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# 4. 벡터 저장소 생성\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m4. 벡터 저장소 생성 중...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m vector_store, documents = \u001b[43mcreate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m    벡터 저장소 생성 완료\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# 5. 질의 실행\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mcreate_vector_store\u001b[39m\u001b[34m(text_chunks, embedding_model)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m vector_store, documents\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m벡터 저장소 생성 실패: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: 벡터 저장소 생성 실패: Error code: 400 - {'error': {'message': 'Requested 313741 tokens, max 300000 tokens per request', 'type': 'max_tokens_per_request', 'param': None, 'code': 'max_tokens_per_request'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OpenAI API 키가 설정되지 않았습니다.\")\n",
    "\n",
    "# 2. 한국어 법률 문서 전용 텍스트 전처리 함수\n",
    "def preprocess_korean_legal_text(text):\n",
    "    \"\"\"한국어 법률 문서를 위한 전처리.\"\"\"\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 조항 번호 정규화 (제1조, 제2조 등)\n",
    "    text = re.sub(r'제(\\d+)조', r'제\\1조', text)\n",
    "    \n",
    "    # 항 번호 정규화\n",
    "    text = re.sub(r'①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩', \n",
    "                  lambda m: f\"제{ord(m.group()) - ord('①') + 1}항\", text)\n",
    "    \n",
    "    # 호 번호 정규화\n",
    "    text = re.sub(r'(\\d+)\\.\\s', r'제\\1호 ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. 개선된 문서 분할 함수\n",
    "def advanced_split_text(text, chunk_size=800, chunk_overlap=200):\n",
    "    \"\"\"법률 문서에 최적화된 텍스트 분할.\"\"\"\n",
    "    # 전처리\n",
    "    text = preprocess_korean_legal_text(text)\n",
    "    \n",
    "    # 법률 문서 구조를 고려한 분할자\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n제\", \"\\n**제\",  # 조항 분할\n",
    "            \"\\n①\", \"\\n②\", \"\\n③\", \"\\n④\", \"\\n⑤\",  # 항 분할\n",
    "            \"\\n1.\", \"\\n2.\", \"\\n3.\", \"\\n4.\", \"\\n5.\",  # 호 분할\n",
    "            \"\\n가.\", \"\\n나.\", \"\\n다.\", \"\\n라.\", \"\\n마.\",  # 목 분할\n",
    "            \"\\n\\n\",  # 문단 분할\n",
    "            \"\\n\",    # 줄 분할\n",
    "            \". \",    # 문장 분할\n",
    "            \" \",     # 단어 분할\n",
    "            \"\"       # 문자 분할\n",
    "        ]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# 4. 개선된 문서 로더\n",
    "def load_docx_advanced(file_path):\n",
    "    \"\"\"개선된 DOCX 파일 로더.\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다.\")\n",
    "        \n",
    "        # 기본 정리\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # 여러 개의 빈 줄을 두 개로 통일\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)  # 여러 공백을 하나로 통일\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "# 5. 벡터 저장소 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "    \"\"\"메타데이터가 포함된 벡터 저장소 생성.\"\"\"\n",
    "    try:\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # 메타데이터 추가\n",
    "            metadata = {\n",
    "                'chunk_id': i,\n",
    "                'chunk_length': len(chunk),\n",
    "                'chunk_type': 'legal_document'\n",
    "            }\n",
    "            \n",
    "            # 조항 정보 추출\n",
    "            if '제' in chunk and '조' in chunk:\n",
    "                article_match = re.search(r'제(\\d+)조', chunk)\n",
    "                if article_match:\n",
    "                    metadata['article'] = f\"제{article_match.group(1)}조\"\n",
    "            \n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        # \n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "        return vector_store, documents\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "# 6. 키워드 기반 검색 함수\n",
    "def keyword_search(query, documents, k=5):\n",
    "    \"\"\"간단한 키워드 기반 검색.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서의 점수 계산\n",
    "    scores = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        content_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 교집합 단어 수로 점수 계산\n",
    "        intersection = query_words.intersection(content_words)\n",
    "        score = len(intersection) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # 정확한 구문 매칭 보너스\n",
    "        if query.lower() in doc.page_content.lower():\n",
    "            score += 0.5\n",
    "        \n",
    "        scores.append((score, i, doc))\n",
    "    \n",
    "    # 점수순 정렬하여 상위 k개 반환\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, _, doc in scores[:k]]\n",
    "\n",
    "# 7. 하이브리드 검색 함수\n",
    "def hybrid_search(query, vector_store, documents, k=5, alpha=0.7):\n",
    "    \"\"\"벡터 검색과 키워드 검색을 결합한 하이브리드 검색.\"\"\"\n",
    "    \n",
    "    # 1. 벡터 유사도 검색\n",
    "    vector_results = vector_store.similarity_search(query, k=k*2)  # 더 많이 가져와서 다양성 확보\n",
    "    \n",
    "    # 2. 키워드 검색\n",
    "    keyword_results = keyword_search(query, documents, k=k*2)\n",
    "    \n",
    "    # 3. 결과 합치기 및 점수 계산\n",
    "    combined_results = {}\n",
    "    \n",
    "    # 벡터 검색 결과 점수 (alpha 가중치)\n",
    "    for i, doc in enumerate(vector_results):\n",
    "        doc_id = doc.page_content\n",
    "        vector_score = alpha * (1.0 - i / len(vector_results))\n",
    "        combined_results[doc_id] = {\n",
    "            'document': doc,\n",
    "            'score': vector_score,\n",
    "            'vector_rank': i + 1\n",
    "        }\n",
    "    \n",
    "    # 키워드 검색 결과 점수 ((1-alpha) 가중치)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc.page_content\n",
    "        keyword_score = (1 - alpha) * (1.0 - i / len(keyword_results))\n",
    "        \n",
    "        if doc_id in combined_results:\n",
    "            # 이미 있는 문서면 점수 합산\n",
    "            combined_results[doc_id]['score'] += keyword_score\n",
    "            combined_results[doc_id]['keyword_rank'] = i + 1\n",
    "        else:\n",
    "            # 새로운 문서면 추가\n",
    "            combined_results[doc_id] = {\n",
    "                'document': doc,\n",
    "                'score': keyword_score,\n",
    "                'keyword_rank': i + 1\n",
    "            }\n",
    "    \n",
    "    # 4. 점수순으로 정렬하여 상위 k개 반환\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [result['document'] for result in sorted_results[:k]]\n",
    "\n",
    "# 8. 한국어 법률 문서 전용 프롬프트 생성 함수\n",
    "def create_korean_legal_prompt():\n",
    "    \"\"\"한국어 법률 문서용 프롬프트 템플릿.\"\"\"\n",
    "    template = \"\"\"당신은 한국 세법 전문가입니다. 주어진 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해야 합니다.\n",
    "\n",
    "다음 규칙을 반드시 따르세요:\n",
    "1. 법조문의 조항, 항, 호, 목을 정확히 인용하세요\n",
    "2. 전문 용어를 사용할 때는 쉬운 설명을 함께 제공하세요\n",
    "3. 관련 조항들 간의 연관성을 설명하세요\n",
    "4. 실무적 적용 방법도 함께 설명하세요\n",
    "5. 불확실한 내용이 있으면 명시적으로 언급하세요\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "위 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해주세요. 관련 조항을 인용하며 설명해주세요.\"\"\"\n",
    "\n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "# 9. 질문 응답 함수\n",
    "def query_with_llm(query, vector_store, documents):\n",
    "    \"\"\"개선된 LLM 기반 질문 응답.\"\"\"\n",
    "    try:\n",
    "        # LLM 모델 설정\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o-mini\",  # 더 강력한 모델 사용\n",
    "            temperature=0.1,  # 일관성 있는 답변을 위해 낮은 temperature\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        print(f\"질의: {query}\")\n",
    "        print(\"하이브리드 검색 수행 중...\")\n",
    "        \n",
    "        # 하이브리드 검색으로 관련 문서 찾기\n",
    "        relevant_docs = hybrid_search(query, vector_store, documents, k=7, alpha=0.7)\n",
    "        \n",
    "        print(f\"검색된 관련 문서: {len(relevant_docs)}개\")\n",
    "        \n",
    "        # 컨텍스트 구성\n",
    "        context = \"\\n\\n\".join([f\"[문서 {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # 한국어 법률 문서용 프롬프트\n",
    "        prompt = create_korean_legal_prompt()\n",
    "        \n",
    "        # 프롬프트 생성\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        print(\"LLM 응답 생성 중...\")\n",
    "        \n",
    "        # LLM 응답 생성\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": relevant_docs,\n",
    "            \"context_used\": context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "# 10. 컨텍스트 품질 평가 함수\n",
    "def evaluate_context_quality(query, retrieved_docs):\n",
    "    \"\"\"검색된 문서의 품질을 간단히 평가.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    quality_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 키워드 매칭 점수\n",
    "        keyword_match = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "        \n",
    "        # 문서 길이 점수 (너무 짧거나 길지 않은 것이 좋음)\n",
    "        length_score = min(len(doc.page_content) / 1000, 1.0)\n",
    "        \n",
    "        # 종합 점수\n",
    "        total_score = (keyword_match * 0.7) + (length_score * 0.3)\n",
    "        quality_scores.append(total_score)\n",
    "    \n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    return avg_quality\n",
    "\n",
    "# 실행 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # DOCX 파일 경로\n",
    "    docx_path = \"data/tax_with_table.docx\"\n",
    "    \n",
    "    print(\"=== 개선된 RAG 파이프라인 실행 ===\\n\")\n",
    "    \n",
    "    # 1. 문서 로드\n",
    "    print(\"1. 문서 로드 중...\")\n",
    "    text = load_docx_advanced(docx_path)\n",
    "    print(f\"   문서 로드 완료: {len(text):,} 문자\\n\")\n",
    "    \n",
    "    # 2. 개선된 문서 분할\n",
    "    print(\"2. 문서 분할 중...\")\n",
    "    text_chunks = advanced_split_text(text, chunk_size=800, chunk_overlap=200)\n",
    "    print(f\"   문서 분할 완료: {len(text_chunks)}개 청크 생성\\n\")\n",
    "    \n",
    "    # 3. 임베딩 모델 초기화\n",
    "    print(\"3. 임베딩 모델 초기화...\")\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-large\",  # 더 성능이 좋은 임베딩 모델\n",
    "    )\n",
    "    print(\"   임베딩 모델 초기화 완료\\n\")\n",
    "    \n",
    "    # 4. 벡터 저장소 생성\n",
    "    print(\"4. 벡터 저장소 생성 중...\")\n",
    "    vector_store, documents = create_vector_store(text_chunks, embedding_model)\n",
    "    print(\"    벡터 저장소 생성 완료\\n\")\n",
    "    \n",
    "    # 5. 질의 실행\n",
    "    print(\"5. 질의 실행 중...\")\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6. 컨텍스트 품질 평가\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7. 결과 출력\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" AI의 답변:\")\n",
    "    print(\"=\"*60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" 검색 결과 요약:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"• 참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "    print(f\"• 컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "    print(f\"• 총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📄 참고한 문서 미리보기:\")\n",
    "    print(\"=\"*60)\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):  # 상위 3개만 미리보기\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개선된 RAG 파이프라인 실행\n",
      "==================================================\n",
      "1. 문서 로드 중...\n",
      "   문서 로드 완료: 289,214 문자\n",
      "\n",
      "2. 문서 분할 중...\n",
      "   문서 분할 완료: 662개 청크 생성\n",
      "   평균 청크 길이: 472자, 최대 길이: 600자\n",
      "\n",
      "3. 임베딩 모델 초기화...\n",
      "   임베딩 모델 초기화 완료\n",
      "\n",
      "4. 벡터 저장소 생성 중...\n",
      "   총 662개 청크를 30개씩 배치 처리...\n",
      "   배치 1/23 처리 중... (30개 문서)\n",
      "   배치 2/23 처리 중... (30개 문서)\n",
      "   배치 3/23 처리 중... (30개 문서)\n",
      "   배치 4/23 처리 중... (30개 문서)\n",
      "   배치 5/23 처리 중... (30개 문서)\n",
      "   배치 6/23 처리 중... (30개 문서)\n",
      "   배치 7/23 처리 중... (30개 문서)\n",
      "   배치 8/23 처리 중... (30개 문서)\n",
      "   배치 9/23 처리 중... (30개 문서)\n",
      "   배치 10/23 처리 중... (30개 문서)\n",
      "   배치 11/23 처리 중... (30개 문서)\n",
      "   배치 12/23 처리 중... (30개 문서)\n",
      "   배치 13/23 처리 중... (30개 문서)\n",
      "   배치 14/23 처리 중... (30개 문서)\n",
      "   배치 15/23 처리 중... (30개 문서)\n",
      "   배치 16/23 처리 중... (30개 문서)\n",
      "   배치 17/23 처리 중... (30개 문서)\n",
      "   배치 18/23 처리 중... (30개 문서)\n",
      "   배치 19/23 처리 중... (30개 문서)\n",
      "   배치 20/23 처리 중... (30개 문서)\n",
      "   배치 21/23 처리 중... (30개 문서)\n",
      "   배치 22/23 처리 중... (30개 문서)\n",
      "   배치 23/23 처리 중... (2개 문서)\n",
      "   모든 배치 처리 완료\n",
      "   벡터 저장소 생성 완료\n",
      "\n",
      "5. 질의 실행 중...\n",
      "질의: 총수입금액 불산입에 대하여 설명해 주세요.\n",
      "하이브리드 검색 수행 중...\n",
      "검색된 관련 문서: 7개\n",
      "LLM 응답 생성 중...\n",
      "\n",
      "============================================================\n",
      "AI의 답변:\n",
      "============================================================\n",
      "총수입금액 불산입에 대한 설명은 다음과 같습니다. \n",
      "\n",
      "### 1. 총수입금액의 정의\n",
      "총수입금액은 거주자가 해당 과세기간에 수입하였거나 수입할 금액의 합계액을 의미합니다. 이는 제24조 제1항에서 명시하고 있으며, 총급여액과 총연금액을 포함합니다. 즉, 사업소득을 계산할 때 모든 수입을 포함하여 산정해야 합니다.\n",
      "\n",
      "### 2. 총수입금액 불산입의 경우\n",
      "총수입금액에서 불산입되는 항목은 다음과 같습니다:\n",
      "\n",
      "- **필요경비 불산입**: 제33조 제1항에 따르면, 거주자가 해당 과세기간에 지급하였거나 지급할 금액 중 특정 항목은 사업소득금액을 계산할 때 필요경비에 산입하지 않습니다. 예를 들어, 업무용 승용차 관련 비용 중 대통령령으로 정하는 업무사용금액에 해당하지 않는 금액은 필요경비로 인정되지 않습니다.\n",
      "\n",
      "- **환급금 및 가산금**: 제8항 및 제9항에 따르면, 국세환급가산금, 지방세환급가산금, 과오납금의 환급금에 대한 이자는 총수입금액에 산입하지 않습니다. 또한, 부가가치세의 매출세액도 총수입금액에 포함되지 않습니다.\n",
      "\n",
      "- **무상으로 받은 자산**: 제6항에 따르면, 거주자가 무상으로 받은 자산의 가액은 총수입금액에 산입하지 않으며, 이는 사업소득을 계산할 때 고려되지 않습니다.\n",
      "\n",
      "### 3. 실무적 적용 방법\n",
      "실무적으로 총수입금액을 계산할 때는 다음과 같은 절차를 따릅니다:\n",
      "\n",
      "1. **모든 수입 항목 확인**: 해당 과세기간에 발생한 모든 수입을 확인합니다. 여기에는 매출, 기타 수입 등이 포함됩니다.\n",
      "   \n",
      "2. **불산입 항목 식별**: 위에서 언급한 불산입 항목을 식별하여 총수입금액에서 제외합니다. 예를 들어, 환급금이나 부가가치세 매출세액은 반드시 제외해야 합니다.\n",
      "\n",
      "3. **필요경비 계산**: 필요경비를 계산할 때는 불산입 항목을 제외한 후, 일반적으로 용인되는 통상적인 비용을 합산하여 사업소득금액을 산정합니다.\n",
      "\n",
      "### 4. 관련 조항 간의 연관성\n",
      "총수입금액의 불산입은 필요경비와 밀접한 관계가 있습니다. 필요경비는 사업소득을 계산할 때 차감되는 비용으로, 불산입되는 항목이 많을수록 최종 소득금액에 영향을 미치게 됩니다. 따라서, 제33조와 제24조는 서로 연결되어 있으며, 총수입금액을 정확히 산정하기 위해서는 필요경비의 불산입 항목을 명확히 이해해야 합니다.\n",
      "\n",
      "### 5. 불확실한 내용\n",
      "법률 문서에서 언급된 특정 세액이나 항목에 대한 대통령령의 규정은 구체적으로 명시되어 있지 않으므로, 해당 규정이 어떻게 적용되는지에 대한 불확실성이 존재합니다. 따라서, 구체적인 사례에 대해서는 관련 법령이나 세무 전문가의 상담을 받는 것이 필요합니다. \n",
      "\n",
      "이와 같이 총수입금액의 불산입에 대한 내용을 정리할 수 있습니다. 추가적인 질문이 있으시면 언제든지 문의해 주세요.\n",
      "\n",
      "============================================================\n",
      "검색 결과 요약:\n",
      "============================================================\n",
      "참고한 문서 조각 수: 7개\n",
      "컨텍스트 품질 점수: 0.19/1.00\n",
      "총 컨텍스트 길이: 3,108 문자\n",
      "\n",
      "============================================================\n",
      "참고한 문서 미리보기:\n",
      "============================================================\n",
      "\n",
      "[문서 1] . 제3항 제1항에 따른 필요경비 불산입에 관하여 필요한 사항은 대통령령으로 정한다. [전문개정 제2009호 제12호 31.] [시행일: 제2025호 제1호 1.] 제33조제1항제1호 제33조의2(업무용승용차 관련 비용 등의 필요경비 불산입 특례) 제1항 제160조제3항에 따른 복식부기의무자가 해당 과세기간에 업무에 사용한 「개별소비세법」 제1조제2항제3호...\n",
      "\n",
      "[문서 2] . 다만, 원재료, 연료, 그 밖의 물품을 매입ㆍ수입 또는 사용함에 따라 부담하는 세액은 그러하지 아니하다. 제8항 「국세기본법」 제52조에 따른 국세환급가산금, 「지방세기본법」 제62조에 따른 지방세환급가산금, 그 밖의 과오납금(過誤納金)의 환급금에 대한 이자는 해당 과세기간의 소득금액을 계산할 때 총수입금액에 산입하지 아니한다.<개정 제2010호 제12...\n",
      "\n",
      "[문서 3] 따라 계산한 금액 제2호 수입금액별 한도: 해당 사업에 대한 해당 과세기간의 수입금액(대통령령으로 정하는 수입금액만 해당한다) 합계액에 다음 표에 규정된 적용률을 적용하여 산출한 금액\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. 환경 변수 로드 및 API 키 설정\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OpenAI API 키가 설정되지 않았습니다. .env 파일에 OPENAI_API_KEY를 추가하세요.\")\n",
    "\n",
    "# 2. 한국어 법률 문서 전용 텍스트 전처리 함수\n",
    "def preprocess_korean_legal_text(text):\n",
    "    \"\"\"\n",
    "    한국어 법률 문서의 구조를 고려한 텍스트 전처리 함수\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원본 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 전처리된 텍스트\n",
    "        \n",
    "    주요 처리 내용:\n",
    "        - 불필요한 공백 및 개행 정리\n",
    "        - 법조문 번호 정규화 (제1조, 제2조 등)\n",
    "        - 항 번호를 아라비아 숫자로 변환 (①→제1항)\n",
    "        - 호 번호 정규화\n",
    "    \"\"\"\n",
    "    # 연속된 공백을 하나의 공백으로 통일\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 조항 번호 정규화: \"제 1 조\" -> \"제1조\" 형태로 통일\n",
    "    text = re.sub(r'제(\\d+)조', r'제\\1조', text)\n",
    "    \n",
    "    # 원문자 항 번호를 아라비아 숫자로 변환하여 검색 정확도 향상\n",
    "    # ①②③④⑤⑥⑦⑧⑨⑩ -> 제1항, 제2항, ... 제10항\n",
    "    text = re.sub(r'①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩', \n",
    "                  lambda m: f\"제{ord(m.group()) - ord('①') + 1}항\", text)\n",
    "    \n",
    "    # 호 번호 정규화: \"1. \" -> \"제1호 \" 형태로 변환\n",
    "    text = re.sub(r'(\\d+)\\.\\s', r'제\\1호 ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. 법률 문서에 최적화된 텍스트 분할 함수\n",
    "def advanced_split_text(text, chunk_size=600, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    법률 문서의 구조적 특성을 고려한 지능적 텍스트 분할\n",
    "    \n",
    "    Args:\n",
    "        text (str): 분할할 텍스트\n",
    "        chunk_size (int): 각 청크의 목표 크기 (문자 수)\n",
    "        chunk_overlap (int): 청크 간 중복되는 문자 수\n",
    "        \n",
    "    Returns:\n",
    "        list: 분할된 텍스트 청크들의 리스트\n",
    "        \n",
    "    특징:\n",
    "        - 법률 문서의 계층 구조(조>항>호>목)를 고려한 분할 우선순위\n",
    "        - 의미적 완성도를 유지하면서 분할\n",
    "        - 토큰 한도를 고려한 적절한 크기 설정\n",
    "    \"\"\"\n",
    "    # 텍스트 전처리 수행\n",
    "    text = preprocess_korean_legal_text(text)\n",
    "    \n",
    "    # 법률 문서 구조를 고려한 분할 구분자들을 우선순위대로 설정\n",
    "    # 상위 구조부터 하위 구조 순서로 분할을 시도\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n제\", \"\\n**제\",  # 조항 단위 분할 (가장 우선)\n",
    "            \"\\n①\", \"\\n②\", \"\\n③\", \"\\n④\", \"\\n⑤\",  # 항 단위 분할\n",
    "            \"\\n1.\", \"\\n2.\", \"\\n3.\", \"\\n4.\", \"\\n5.\",  # 호 단위 분할\n",
    "            \"\\n가.\", \"\\n나.\", \"\\n다.\", \"\\n라.\", \"\\n마.\",  # 목 단위 분할\n",
    "            \"\\n\\n\",  # 문단 단위 분할\n",
    "            \"\\n\",    # 줄 단위 분할\n",
    "            \". \",    # 문장 단위 분할\n",
    "            \" \",     # 단어 단위 분할\n",
    "            \"\"       # 문자 단위 분할 (최후 수단)\n",
    "        ]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# 4. DOCX 파일 로딩 및 전처리 함수\n",
    "def load_docx_advanced(file_path):\n",
    "    \"\"\"\n",
    "    DOCX 파일을 로드하고 기본적인 텍스트 정리를 수행\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): DOCX 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        str: 정리된 텍스트\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: 파일 로딩 실패 시\n",
    "        ValueError: 텍스트 추출 실패 시\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Docx2txtLoader를 사용하여 DOCX 파일에서 텍스트 추출\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        # 텍스트가 비어있는지 확인\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다. 파일이 비어있거나 손상되었을 수 있습니다.\")\n",
    "        \n",
    "        # 기본적인 텍스트 정리 작업\n",
    "        # 연속된 빈 줄을 두 개의 줄바꿈으로 통일\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        # 연속된 공백과 탭을 하나의 공백으로 통일\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "# 5. 배치 처리를 통한 벡터 저장소 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model, batch_size=30):\n",
    "    \"\"\"\n",
    "    텍스트 청크들을 배치 단위로 처리하여 벡터 저장소 생성\n",
    "    토큰 한도 초과 문제를 해결하기 위해 배치 처리 방식 적용\n",
    "    \n",
    "    Args:\n",
    "        text_chunks (list): 분할된 텍스트 청크들\n",
    "        embedding_model: OpenAI 임베딩 모델 객체\n",
    "        batch_size (int): 한 번에 처리할 청크 수 (기본값: 30)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (FAISS 벡터 저장소, Document 객체들의 리스트)\n",
    "        \n",
    "    처리 과정:\n",
    "        1. 각 청크에 메타데이터 추가 (ID, 길이, 조항 정보 등)\n",
    "        2. 청크 크기가 너무 큰 경우 자동으로 제한\n",
    "        3. 배치 단위로 임베딩 생성하여 토큰 한도 문제 방지\n",
    "        4. FAISS merge 기능을 활용하여 배치별 결과 통합\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"   총 {len(text_chunks)}개 청크를 {batch_size}개씩 배치 처리...\")\n",
    "        \n",
    "        # Document 객체들을 저장할 리스트 초기화\n",
    "        documents = []\n",
    "        \n",
    "        # 각 텍스트 청크를 Document 객체로 변환하면서 메타데이터 추가\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # 청크 크기가 너무 큰 경우 제한 (토큰 한도 방지)\n",
    "            if len(chunk) > 2000:\n",
    "                chunk = chunk[:2000] + \"...\"\n",
    "                print(f\"   경고: 청크 {i}가 너무 커서 2000자로 제한했습니다.\")\n",
    "            \n",
    "            # 각 청크에 추가할 메타데이터 구성\n",
    "            metadata = {\n",
    "                'chunk_id': i,  # 청크 고유 번호\n",
    "                'chunk_length': len(chunk),  # 청크 길이\n",
    "                'chunk_type': 'legal_document'  # 문서 유형\n",
    "            }\n",
    "            \n",
    "            # 청크 내용에서 조항 정보 자동 추출\n",
    "            # \"제n조\" 패턴을 찾아 메타데이터에 추가\n",
    "            if '제' in chunk and '조' in chunk:\n",
    "                article_match = re.search(r'제(\\d+)조', chunk)\n",
    "                if article_match:\n",
    "                    metadata['article'] = f\"제{article_match.group(1)}조\"\n",
    "            \n",
    "            # Document 객체 생성하여 리스트에 추가\n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        \n",
    "        # 배치별 벡터 저장소 생성 및 병합 과정\n",
    "        vector_store = None\n",
    "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # 문서들을 배치 크기만큼 나누어 처리\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch_docs = documents[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            \n",
    "            print(f\"   배치 {batch_num}/{total_batches} 처리 중... ({len(batch_docs)}개 문서)\")\n",
    "            \n",
    "            # 첫 번째 배치인 경우 새로운 벡터 저장소 생성\n",
    "            if vector_store is None:\n",
    "                vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "            else:\n",
    "                # 이후 배치들은 기존 벡터 저장소에 병합\n",
    "                batch_vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "                vector_store.merge_from(batch_vector_store)\n",
    "        \n",
    "        print(\"   모든 배치 처리 완료\")\n",
    "        return vector_store, documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "# 6. 키워드 기반 검색 함수\n",
    "def keyword_search(query, documents, k=5):\n",
    "    \"\"\"\n",
    "    단순한 키워드 매칭을 통한 문서 검색\n",
    "    벡터 검색과 상호 보완적으로 사용하여 검색 정확도 향상\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 질의\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        k (int): 반환할 상위 문서 수\n",
    "        \n",
    "    Returns:\n",
    "        list: 관련도 순으로 정렬된 Document 객체들\n",
    "        \n",
    "    검색 로직:\n",
    "        1. 질의와 문서의 단어 교집합 계산\n",
    "        2. 교집합 크기를 질의 단어 수로 나누어 정규화\n",
    "        3. 정확한 구문 매칭 시 보너스 점수 부여\n",
    "        4. 점수 기준으로 상위 k개 문서 반환\n",
    "    \"\"\"\n",
    "    # 질의를 소문자로 변환하고 단어 단위로 분할\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서의 점수를 계산할 리스트\n",
    "    scores = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # 문서 내용을 소문자로 변환하고 단어 단위로 분할\n",
    "        content_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 질의 단어와 문서 단어의 교집합 계산\n",
    "        intersection = query_words.intersection(content_words)\n",
    "        \n",
    "        # 기본 점수: 교집합 크기를 질의 단어 수로 나누어 정규화 (0~1 범위)\n",
    "        score = len(intersection) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # 보너스 점수: 질의 전체가 문서에 정확히 포함된 경우\n",
    "        if query.lower() in doc.page_content.lower():\n",
    "            score += 0.5\n",
    "        \n",
    "        # (점수, 인덱스, 문서) 튜플로 저장\n",
    "        scores.append((score, i, doc))\n",
    "    \n",
    "    # 점수 기준으로 내림차순 정렬하여 상위 k개 반환\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, _, doc in scores[:k]]\n",
    "\n",
    "# 7. 하이브리드 검색 함수 (벡터 + 키워드 검색 결합)\n",
    "def hybrid_search(query, vector_store, documents, k=5, alpha=0.7):\n",
    "    \"\"\"\n",
    "    벡터 유사도 검색과 키워드 검색을 결합한 하이브리드 검색\n",
    "    두 검색 방법의 장점을 결합하여 더 정확한 검색 결과 제공\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 질의\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        k (int): 최종 반환할 문서 수\n",
    "        alpha (float): 벡터 검색 가중치 (0~1, 높을수록 벡터 검색 중시)\n",
    "        \n",
    "    Returns:\n",
    "        list: 종합 점수로 정렬된 상위 k개 Document 객체들\n",
    "        \n",
    "    검색 과정:\n",
    "        1. 벡터 유사도 검색으로 의미적으로 관련된 문서 찾기\n",
    "        2. 키워드 검색으로 정확한 용어 매칭 문서 찾기\n",
    "        3. 두 결과를 alpha 가중치로 결합\n",
    "        4. 중복 문서 처리 및 최종 점수 계산\n",
    "        5. 점수 순으로 정렬하여 상위 k개 반환\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 벡터 유사도 검색 수행\n",
    "    # 더 많은 후보를 가져와서 다양성 확보 (k*2개)\n",
    "    vector_results = vector_store.similarity_search(query, k=k*2)\n",
    "    \n",
    "    # 2. 키워드 기반 검색 수행\n",
    "    keyword_results = keyword_search(query, documents, k=k*2)\n",
    "    \n",
    "    # 3. 두 검색 결과를 점수와 함께 통합\n",
    "    combined_results = {}\n",
    "    \n",
    "    # 벡터 검색 결과에 점수 부여 (alpha 가중치 적용)\n",
    "    for i, doc in enumerate(vector_results):\n",
    "        # 문서 내용을 고유 키로 사용\n",
    "        doc_id = doc.page_content\n",
    "        # 순위가 높을수록 높은 점수 (1.0에서 시작하여 순위에 따라 감소)\n",
    "        vector_score = alpha * (1.0 - i / len(vector_results))\n",
    "        \n",
    "        combined_results[doc_id] = {\n",
    "            'document': doc,\n",
    "            'score': vector_score,\n",
    "            'vector_rank': i + 1,\n",
    "            'keyword_rank': None\n",
    "        }\n",
    "    \n",
    "    # 키워드 검색 결과에 점수 부여 ((1-alpha) 가중치 적용)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc.page_content\n",
    "        keyword_score = (1 - alpha) * (1.0 - i / len(keyword_results))\n",
    "        \n",
    "        if doc_id in combined_results:\n",
    "            # 이미 벡터 검색에서 찾은 문서인 경우 점수 합산\n",
    "            combined_results[doc_id]['score'] += keyword_score\n",
    "            combined_results[doc_id]['keyword_rank'] = i + 1\n",
    "        else:\n",
    "            # 키워드 검색에서만 찾은 새로운 문서인 경우 추가\n",
    "            combined_results[doc_id] = {\n",
    "                'document': doc,\n",
    "                'score': keyword_score,\n",
    "                'vector_rank': None,\n",
    "                'keyword_rank': i + 1\n",
    "            }\n",
    "    \n",
    "    # 4. 종합 점수 기준으로 정렬하여 상위 k개 반환\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [result['document'] for result in sorted_results[:k]]\n",
    "\n",
    "# 8. 한국어 법률 문서 전용 프롬프트 생성 함수\n",
    "def create_korean_legal_prompt():\n",
    "    \"\"\"\n",
    "    한국어 법률 문서 특성에 맞춘 전용 프롬프트 템플릿 생성\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: 법률 문서 질의응답을 위한 프롬프트 템플릿\n",
    "        \n",
    "    프롬프트 특징:\n",
    "        - 법조문 인용의 정확성 강조\n",
    "        - 전문 용어에 대한 쉬운 설명 요구\n",
    "        - 조항 간 연관성 설명 포함\n",
    "        - 실무적 적용 방법 제시\n",
    "        - 불확실한 내용에 대한 명시적 언급\n",
    "    \"\"\"\n",
    "    template = \"\"\"당신은 한국 세법 전문가입니다. 주어진 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해야 합니다.\n",
    "\n",
    "다음 규칙을 반드시 따르세요:\n",
    "1. 법조문의 조항, 항, 호, 목을 정확히 인용하세요\n",
    "2. 전문 용어를 사용할 때는 쉬운 설명을 함께 제공하세요\n",
    "3. 관련 조항들 간의 연관성을 설명하세요\n",
    "4. 실무적 적용 방법도 함께 설명하세요\n",
    "5. 불확실한 내용이 있으면 명시적으로 언급하세요\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "위 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해주세요. 관련 조항을 인용하며 설명해주세요.\"\"\"\n",
    "\n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "# 9. LLM을 활용한 질문 응답 함수\n",
    "def query_with_llm(query, vector_store, documents):\n",
    "    \"\"\"\n",
    "    하이브리드 검색과 고성능 LLM을 결합한 질문 응답 시스템\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        \n",
    "    Returns:\n",
    "        dict: 답변, 참고 문서, 사용된 컨텍스트를 포함한 응답 딕셔너리\n",
    "        \n",
    "    처리 과정:\n",
    "        1. GPT-4o-mini 모델로 LLM 초기화 (높은 정확도)\n",
    "        2. 하이브리드 검색으로 관련 문서 7개 검색\n",
    "        3. 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        4. 법률 문서 전용 프롬프트 적용\n",
    "        5. LLM으로 최종 답변 생성\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 고성능 LLM 모델 설정\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o-mini\",  # 정확도가 높은 모델 사용\n",
    "            temperature=0.1,  # 낮은 온도로 일관성 있는 답변 생성\n",
    "            max_tokens=1000   # 충분한 답변 길이 허용\n",
    "        )\n",
    "        \n",
    "        print(f\"질의: {query}\")\n",
    "        print(\"하이브리드 검색 수행 중...\")\n",
    "        \n",
    "        # 하이브리드 검색으로 관련 문서 검색\n",
    "        # k=7로 설정하여 충분한 컨텍스트 확보\n",
    "        # alpha=0.7로 설정하여 벡터 검색을 더 중시 (의미적 유사도 우선)\n",
    "        relevant_docs = hybrid_search(query, vector_store, documents, k=7, alpha=0.7)\n",
    "        \n",
    "        print(f\"검색된 관련 문서: {len(relevant_docs)}개\")\n",
    "        \n",
    "        # 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        # 각 문서에 번호를 매겨 구분하기 쉽게 구성\n",
    "        context = \"\\n\\n\".join([f\"[문서 {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # 한국어 법률 문서에 특화된 프롬프트 사용\n",
    "        prompt = create_korean_legal_prompt()\n",
    "        \n",
    "        # 최종 프롬프트 생성 (컨텍스트와 질문 삽입)\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        print(\"LLM 응답 생성 중...\")\n",
    "        \n",
    "        # LLM에 프롬프트 전달하여 답변 생성\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # 결과를 딕셔너리 형태로 반환\n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": relevant_docs,\n",
    "            \"context_used\": context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "# 10. 검색 품질 평가 함수\n",
    "def evaluate_context_quality(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    검색된 문서들의 품질을 정량적으로 평가\n",
    "    \n",
    "    Args:\n",
    "        query (str): 원본 질의\n",
    "        retrieved_docs (list): 검색된 Document 객체들\n",
    "        \n",
    "    Returns:\n",
    "        float: 0~1 범위의 품질 점수 (1에 가까울수록 높은 품질)\n",
    "        \n",
    "    평가 기준:\n",
    "        1. 키워드 매칭률 (70% 가중치): 질의 단어가 문서에 포함된 비율\n",
    "        2. 문서 길이 적절성 (30% 가중치): 너무 짧거나 길지 않은 적절한 길이\n",
    "    \"\"\"\n",
    "    # 질의를 단어 단위로 분할하고 소문자 변환\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서별 품질 점수 계산\n",
    "    quality_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        # 문서 내용을 단어 단위로 분할하고 소문자 변환\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 키워드 매칭 점수 계산\n",
    "        # 질의 단어 중 문서에 포함된 단어의 비율\n",
    "        keyword_match = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "        \n",
    "        # 문서 길이 점수 계산\n",
    "        # 1000자를 기준으로 정규화 (1000자 이상이면 1.0점)\n",
    "        length_score = min(len(doc.page_content) / 1000, 1.0)\n",
    "        \n",
    "        # 종합 점수 계산 (키워드 매칭 70% + 길이 적절성 30%)\n",
    "        total_score = (keyword_match * 0.7) + (length_score * 0.3)\n",
    "        quality_scores.append(total_score)\n",
    "    \n",
    "    # 전체 문서의 평균 품질 점수 반환\n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    return avg_quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개선된 RAG 파이프라인 실행\n",
      "==================================================\n",
      "1. 문서 로드 중...\n",
      "   문서 로드 완료: 289,214 문자\n",
      "\n",
      "2. 문서 분할 중...\n",
      "   문서 분할 완료: 662개 청크 생성\n",
      "   평균 청크 길이: 472자, 최대 길이: 600자\n",
      "\n",
      "3. 임베딩 모델 초기화...\n",
      "   임베딩 모델 초기화 완료\n",
      "\n",
      "4. 벡터 저장소 생성 중...\n",
      "   총 662개 청크를 30개씩 배치 처리...\n",
      "   배치 1/23 처리 중... (30개 문서)\n",
      "   배치 2/23 처리 중... (30개 문서)\n",
      "   배치 3/23 처리 중... (30개 문서)\n",
      "   배치 4/23 처리 중... (30개 문서)\n",
      "   배치 5/23 처리 중... (30개 문서)\n",
      "   배치 6/23 처리 중... (30개 문서)\n",
      "   배치 7/23 처리 중... (30개 문서)\n",
      "   배치 8/23 처리 중... (30개 문서)\n",
      "   배치 9/23 처리 중... (30개 문서)\n",
      "   배치 10/23 처리 중... (30개 문서)\n",
      "   배치 11/23 처리 중... (30개 문서)\n",
      "   배치 12/23 처리 중... (30개 문서)\n",
      "   배치 13/23 처리 중... (30개 문서)\n",
      "   배치 14/23 처리 중... (30개 문서)\n",
      "   배치 15/23 처리 중... (30개 문서)\n",
      "   배치 16/23 처리 중... (30개 문서)\n",
      "   배치 17/23 처리 중... (30개 문서)\n",
      "   배치 18/23 처리 중... (30개 문서)\n",
      "   배치 19/23 처리 중... (30개 문서)\n",
      "   배치 20/23 처리 중... (30개 문서)\n",
      "   배치 21/23 처리 중... (30개 문서)\n",
      "   배치 22/23 처리 중... (30개 문서)\n",
      "   배치 23/23 처리 중... (2개 문서)\n",
      "   모든 배치 처리 완료\n",
      "   벡터 저장소 생성 완료\n",
      "\n",
      "5. 질의 실행 중...\n",
      "질의: 비과세소득의 종류에 대하여 설명해 주세요.\n",
      "하이브리드 검색 수행 중...\n",
      "검색된 관련 문서: 7개\n",
      "LLM 응답 생성 중...\n",
      "\n",
      "============================================================\n",
      "AI의 답변:\n",
      "============================================================\n",
      "비과세소득의 종류에 대해서는 주어진 법률 문서에서 직접적으로 언급된 내용은 없지만, 일반적으로 비과세소득은 특정한 법률 조항에 의해 과세되지 않는 소득을 의미합니다. 한국 세법에서 비과세소득에 대한 규정은 여러 법률에 걸쳐 존재하며, 주로 소득세법 및 관련 세법에서 규정됩니다.\n",
      "\n",
      "### 비과세소득의 일반적인 예시\n",
      "\n",
      "1. **상속세 및 증여세**: 상속이나 증여로 인한 소득은 일반적으로 소득세의 과세 대상이 아닙니다. 이는 상속세 및 증여세법에 의해 별도로 과세됩니다.\n",
      "\n",
      "2. **특정한 정부 보조금**: 정부에서 지급하는 특정 보조금이나 지원금은 비과세소득으로 간주될 수 있습니다. 예를 들어, 재난지원금이나 특정 조건을 충족하는 사회복지 혜택 등이 이에 해당합니다.\n",
      "\n",
      "3. **이자소득의 비과세**: 특정 금융상품에서 발생하는 이자소득은 비과세 혜택을 받을 수 있습니다. 예를 들어, 특정 조건을 충족하는 저축성 보험의 이자소득은 비과세로 처리될 수 있습니다.\n",
      "\n",
      "4. **퇴직소득**: 퇴직금은 일정 금액 이하의 경우 비과세로 처리될 수 있습니다. 이는 소득세법 제2조 제2호에 명시되어 있습니다.\n",
      "\n",
      "5. **기타소득의 비과세**: 특정 조건을 충족하는 기타소득도 비과세로 처리될 수 있습니다. 예를 들어, 소액의 경조사비나 특정한 조건을 충족하는 상금 등이 이에 해당할 수 있습니다.\n",
      "\n",
      "### 관련 조항 인용 및 설명\n",
      "\n",
      "- **소득세법 제4조(소득의 구분)**: 이 조항에서는 거주자의 소득을 여러 가지로 구분하고 있으며, 비과세소득에 대한 명확한 규정은 없지만, 특정 소득이 과세되지 않는 경우를 명시할 수 있는 근거가 됩니다.\n",
      "\n",
      "- **소득세법 제27조(사업소득의 필요경비의 계산)**: 이 조항은 사업소득을 계산할 때 필요경비를 산입하는 방법을 규정하고 있으며, 비과세소득과 관련된 경비 처리에 대한 기준을 제공할 수 있습니다.\n",
      "\n",
      "### 실무적 적용 방법\n",
      "\n",
      "비과세소득을 적용하기 위해서는 다음과 같은 절차를 따르는 것이 일반적입니다:\n",
      "\n",
      "1. **소득의 성격 파악**: 소득이 비과세소득에 해당하는지 여부를 판단하기 위해 소득의 발생 원인과 성격을 명확히 파악해야 합니다.\n",
      "\n",
      "2. **관련 법령 검토**: 비과세소득에 해당하는지 확인하기 위해 관련 법령을 검토하고, 필요한 경우 세무 전문가의 자문을 받는 것이 좋습니다.\n",
      "\n",
      "3. **세무 신고 시 반영**: 비과세소득으로 판단된 경우, 세무 신고 시 해당 소득을 제외하고 신고해야 하며, 이를 증명할 수 있는 서류를 준비해야 합니다.\n",
      "\n",
      "### 불확실한 내용\n",
      "\n",
      "비과세소득의 종류와 적용에 대한 구체적인 사항은 법령의 개정이나 해석에 따라 달라질 수 있으므로, 항상 최신 법령을 확인하고, 필요시 세무 전문가와 상담하는 것이 중요합니다.\n",
      "\n",
      "============================================================\n",
      "검색 결과 요약:\n",
      "============================================================\n",
      "참고한 문서 조각 수: 7개\n",
      "컨텍스트 품질 점수: 0.23/1.00\n",
      "총 컨텍스트 길이: 3,956 문자\n",
      "\n",
      "============================================================\n",
      "참고한 문서 미리보기:\n",
      "============================================================\n",
      "\n",
      "[문서 1] . 제2항 제1항에도 불구하고 위탁자가 신탁재산을 실질적으로 통제하는 등 대통령령으로 정하는 요건을 충족하는 신탁의 경우에는 그 신탁재산에 귀속되는 소득은 위탁자에게 귀속되는 것으로 본다.<개정 제2023호 제12호 31.> [본조신설 제2020호 제12호 29.] 제3조(과세소득의 범위) 제1항 거주자에게는 이 법에서 규정하는 모든 소득에 대해서 과세한다...\n",
      "\n",
      "[문서 2] . 이자소득 나. 배당소득 다. 사업소득 라. 근로소득 마. 연금소득 바. 기타소득 제2호 퇴직소득 2의제2호 금융투자소득 제3호 양도소득 제2항 제1항에 따른 소득을 구분할 때 다음 각 호의 신탁을 제외한 신탁의 이익은 「신탁법」 제2조에 따라 수탁자에게 이전되거나 그 밖에 처분된 재산권에서 발생하는 소득의 내용별로 구분한다.<개정 제2011호 제7호 2...\n",
      "\n",
      "[문서 3] . <개정 제2010호 제12호 27., 제2014호 제1호 1., 제2018호 제12호 31., 제2020호 제12호 29., 제2022호 제12호 31.> 제1호 소득세(제57조 및 제57조의2에 따라 세액공제를 적용하는 경우의 외국소득세액을 포함한다)와 개인지방소득세 제2호 벌금ㆍ과료(통고처분에 따른 벌금 또는 과료에 해당하는 금액을 포함한다)와 과태...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    # 처리할 DOCX 파일 경로 설정\n",
    "    docx_path = \"data/tax_with_table.docx\"\n",
    "    \n",
    "    print(\"개선된 RAG 파이프라인 실행\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1단계: 문서 로드\n",
    "    print(\"1. 문서 로드 중...\")\n",
    "    text = load_docx_advanced(docx_path)\n",
    "    print(f\"   문서 로드 완료: {len(text):,} 문자\")\n",
    "    \n",
    "    # 2단계: 문서 분할\n",
    "    print(\"\\n2. 문서 분할 중...\")\n",
    "    text_chunks = advanced_split_text(text, chunk_size=600, chunk_overlap=100)\n",
    "    print(f\"   문서 분할 완료: {len(text_chunks)}개 청크 생성\")\n",
    "    \n",
    "    # 청크 크기 통계 분석 및 출력\n",
    "    chunk_lengths = [len(chunk) for chunk in text_chunks]\n",
    "    avg_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "    max_length = max(chunk_lengths)\n",
    "    print(f\"   평균 청크 길이: {avg_length:.0f}자, 최대 길이: {max_length}자\")\n",
    "    \n",
    "    # 3단계: 임베딩 모델 초기화\n",
    "    print(\"\\n3. 임베딩 모델 초기화...\")\n",
    "    # 성능이 우수한 text-embedding-3-large 모델 사용\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-large\",\n",
    "    )\n",
    "    print(\"   임베딩 모델 초기화 완료\")\n",
    "    \n",
    "    # 4단계: 벡터 저장소 생성\n",
    "    print(\"\\n4. 벡터 저장소 생성 중...\")\n",
    "    # 배치 크기 30으로 설정하여 토큰 한도 문제 방지\n",
    "    vector_store, documents = create_vector_store(text_chunks, embedding_model, batch_size=30)\n",
    "    print(\"   벡터 저장소 생성 완료\")\n",
    "    \n",
    "    # 5단계: 질의 실행\n",
    "    print(\"\\n5. 질의 실행 중...\")\n",
    "   #query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    query = \"비과세소득의 종류에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6단계: 검색 품질 평가\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7단계: 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AI의 답변:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"검색 결과 요약:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "    print(f\"컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "    print(f\"총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"참고한 문서 미리보기:\")\n",
    "    print(\"=\" * 60)\n",
    "    # 상위 3개 문서의 일부만 미리보기로 출력\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):\n",
    "        # 200자까지만 미리보기로 표시\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 5단계: 질의 실행\n",
    "    print(\"\\n5. 질의 실행 중...\")\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6단계: 검색 품질 평가\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7단계: 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AI의 답변:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"검색 결과 요약:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "    print(f\"컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "    print(f\"총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"참고한 문서 미리보기:\")\n",
    "    print(\"=\" * 60)\n",
    "    # 상위 3개 문서의 일부만 미리보기로 출력\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):\n",
    "        # 200자까지만 미리보기로 표시\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-0lCeHk3W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
