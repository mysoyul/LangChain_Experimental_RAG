{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-KIIvQ5NKJP"
   },
   "source": [
    "PDF 파일기반 질의응답 챗봇 (랭체인, 그라디오, ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHgrhVbLnM6I",
    "outputId": "cd8a98a1-5a96-41f1-a643-7eac3418e186"
   },
   "outputs": [],
   "source": [
    "%pip install -q openai\n",
    "%pip install -q langchain\n",
    "%pip install -q -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5uYZQknXLDM"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lSX_ndLWHJqy",
    "outputId": "f09295ac-c9f0-41f2-b199-f32ad1dfc295"
   },
   "outputs": [],
   "source": [
    "%pip install -q pypdf\n",
    "%pip install -q chromadb\n",
    "%pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kZxXmSjQRHy"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VgBZ49vQVzg"
   },
   "outputs": [],
   "source": [
    "# PDF 로드\n",
    "loader = PyPDFLoader(\"data/how-to-use-AI-in-6industries.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VgBZ49vQVzg"
   },
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4otdIuNkT5b_"
   },
   "outputs": [],
   "source": [
    "# OpenAI Embeddings 적용\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Chroma 벡터 저장소 생성\n",
    "vector_store = Chroma.from_documents(texts, embeddings)\n",
    "# 벡터 저장소에서 검색하기\n",
    "# search_type=\"similarity\" 옵션은 retrieval 객체에서 유사성 검색을 사용하여 질문 vector와 가장 유사한 문장 vector를 선택함\n",
    "# search_kwargs 옵션은 vector 저장소에서 Prompt 2개의 텍스트 덩어리로 보내려는 것을 의미함\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# ChatOpenAI는 기본모델인  gpt-3.5-turbo 사용하고\n",
    "# temperature=0은 보수적인 지문에 대한 답변을 내고, temperature=1 다양한 답변을 낼 수 있음 \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# RetrievalQA가 실질적인 RAG를 수행하는 객체\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,  # 기존 retriever 유지\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDW1hZX3a_1w",
    "outputId": "045636e5-2c71-4234-e8bf-dddb206b4d0d"
   },
   "outputs": [],
   "source": [
    "query = \"언더라이팅\"\n",
    "query_result = chain.invoke(query)\n",
    "print(type(query_result))\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in query_result.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(query_result['source_documents']))\n",
    "type(query_result['source_documents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = query_result['source_documents'][0]\n",
    "print(type(doc))\n",
    "doc_dict = doc.model_dump()\n",
    "print(type(doc_dict))\n",
    "print(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in doc_dict:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "Qr0_iNsLTb6y",
    "outputId": "495980f8-d3a4-4fc8-8d56-8e7872378528"
   },
   "outputs": [],
   "source": [
    "doc_dict['metadata']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"소비자 부문 AI 활용사례 무엇인가요?\"\n",
    "query_result = chain.invoke(query)\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "system_template = \"\"\"Use the following pieces of context to answer the user's question shortly.\n",
    "Given the following summaries of a long document and a question, create a final answer with references (\"SOURCES\"), \n",
    "use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\n",
    "\n",
    "You MUST answer in Korean and in Markdown format:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain_type_kwargs = {\n",
    "    \"prompt\": prompt,\n",
    "    \"document_variable_name\": \"summaries\",\n",
    "}\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # GPT-4를 사용하려면 model=\"gpt-4\"\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    input_key=\"question\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVdJrxN0TzwC",
    "outputId": "3c7b9031-8f8f-448d-e509-7cceafc109cd"
   },
   "outputs": [],
   "source": [
    "query = \"소비자 부문 AI 활용사례 무엇인가요?\"\n",
    "query_result = chain.invoke({\"question\": query})\n",
    "print(type(query_result))\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "cM3DcAF4Uqvh",
    "outputId": "282386ab-9a3c-4c16-da35-179ae8f250c2"
   },
   "outputs": [],
   "source": [
    "print(query_result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxNF1I2pUzIv",
    "outputId": "475500a9-42d5-42bf-ff62-e6d07a7d0806"
   },
   "outputs": [],
   "source": [
    "query_result['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSOqIBzXU6f_",
    "outputId": "181d4cbc-166f-4a44-b291-e8b731a7a997"
   },
   "outputs": [],
   "source": [
    "for doc in query_result['source_documents']:\n",
    "    print('내용 : ' + doc.page_content[0:100].replace('\\n', ' '))\n",
    "    print('파일 : ' + doc.metadata['source'])\n",
    "    print('페이지 : ' + str(doc.metadata['page']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_message = query_result['result']\n",
    "for i, doc in enumerate(query_result['source_documents']):\n",
    "    bot_message += '[' + str(i+1) + '] ' + doc.metadata['source'] + '(' + str(doc.metadata['page']) + ') '\n",
    "    print(bot_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHPK_8ZWHCRb",
    "outputId": "d3e45ebd-182c-4a99-ccf0-0031da84e8dd"
   },
   "outputs": [],
   "source": [
    "%pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "Yl_SXA37i4p5",
    "outputId": "432c831e-8b4a-48fd-99b6-d00ee49a9893"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import gradio as gr\n",
    "\n",
    "# 1. PDF를 한 번만 로드하여 벡터 저장소 생성\n",
    "def initialize_retriever():\n",
    "    # PDF 파일을 로드하여 문서 객체로 변환\n",
    "    loader = PyPDFLoader(\"how-to-use-AI-in-6industries.pdf\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 문서를 일정한 크기로 분할 (chunk_size=1000, 중첩 없음)\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # 문서의 텍스트를 벡터 임베딩으로 변환\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # 변환된 벡터를 Chroma 벡터 저장소에 저장\n",
    "    vector_store = Chroma.from_documents(texts, embeddings)\n",
    "    # 저장된 벡터를 검색할 수 있는 retriever 생성 (유사한 문서 2개 검색)\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "    return retriever\n",
    "\n",
    "# 2. 전역 retriever 생성 (앱 시작 시 한 번만 실행)\n",
    "retriever = initialize_retriever()\n",
    "\n",
    "# 3. 채팅 응답 함수 (retriever를 재사용)\n",
    "def chat_respond(message, chat_history):\n",
    "    #  시스템 메시지 템플릿: 문서 요약을 기반으로 질문에 답변하도록 설정\n",
    "    system_template = \"\"\"Use the following pieces of context to answer the user's question shortly.\n",
    "    Given the following summaries of a long document and a question, create a final answer with references (\"SOURCES\"), \n",
    "    use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "    If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "    ----------------\n",
    "    {summaries}\n",
    "    \n",
    "    You MUST answer in Korean and in Markdown format:\n",
    "    \"\"\"\n",
    "    # 사용자 질문을 받아 최종 Prompt 구성\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    chain_type_kwargs = {\n",
    "        \"prompt\": prompt, #  LLM이 사용할 프롬프트 지정\n",
    "        \"document_variable_name\": \"summaries\", #  문서 요약 데이터를 LLM에 전달할 변수 이름\n",
    "    }\n",
    "    \n",
    "    # OpenAI 모델 (GPT-3.5-turbo) 설정\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    \n",
    "    # RetrievalQA 체인 생성\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  #  문서를 하나의 큰 텍스트로 처리하는 방식\n",
    "        retriever=retriever, #  유사 문서를 검색하는 retriever 연결\n",
    "        return_source_documents=True, #  답변에 참조한 문서 정보 포함\n",
    "        chain_type_kwargs=chain_type_kwargs,\n",
    "        input_key=\"question\" #  사용자 질문을 \"question\" 키로 전달\n",
    "    )\n",
    "    \n",
    "    # LLM을 호출하여 질문에 대한 응답 생성\n",
    "    query_result = chain.invoke({\"question\": message})\n",
    "\n",
    "    # 모델의 답변을 가져오기\n",
    "    bot_message = query_result['result']\n",
    "\n",
    "    # 참조한 문서 정보를 응답에 추가\n",
    "    for i, doc in enumerate(query_result['source_documents']):\n",
    "        bot_message += f' [{i+1}] {doc.metadata.get(\"source\", \"Unknown\")} (Page {doc.metadata.get(\"page\", \"N/A\")})'\n",
    "\n",
    "    # Gradio 채팅 기록 형식에 맞춰 응답을 저장\n",
    "    chat_history.append({\"role\": \"user\", \"content\": message})  # 사용자 메시지 추가\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": bot_message})  # 봇 응답 추가\n",
    "    \n",
    "    # 입력 창 초기화 및 갱신된 채팅 기록 반환\n",
    "    return \"\", chat_history\n",
    "\n",
    "# 4. # Gradio UI 생성 및 실행\n",
    "with gr.Blocks() as demo:  \n",
    "    # 채팅 창 (채팅 메시지를 표시하는 Gradio 컴포넌트)\n",
    "    chatbot = gr.Chatbot(label=\"채팅창\", type=\"messages\")\n",
    "    # 사용자 입력 텍스트 박스 (메시지를 입력하는 필드)\n",
    "    msg = gr.Textbox(label=\"입력\")\n",
    "    # 초기화 버튼 (채팅 기록을 초기화하는 버튼)\n",
    "    clear = gr.Button(\"초기화\")\n",
    "\n",
    "    # 사용자가 입력 필드에 메시지를 입력하면 chat_respond()가 실행됨\n",
    "    #  - 입력한 메시지는 msg에서 가져옴\n",
    "    #  - 기존 채팅 기록(chatbot)과 함께 전달됨\n",
    "    #  - 응답을 받은 후, msg를 초기화하고 chatbot에 대화 내용을 추가\n",
    "    msg.submit(chat_respond, [msg, chatbot], [msg, chatbot])  \n",
    "    # 초기화 버튼 클릭 시, 채팅 기록을 비우도록 설정\n",
    "    clear.click(lambda: [], None, chatbot, queue=False)\n",
    "\n",
    "# Gradio 앱 실행 (debug 모드 활성화)\n",
    "demo.launch(debug=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8LrpDnskG7y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chatbot-0lCeHk3W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
