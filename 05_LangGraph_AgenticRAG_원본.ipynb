{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e66a4e",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a539f",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a27628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394ecdf",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe5c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, json\n",
    "\n",
    "from textwrap import dedent\n",
    "from pprint import pprint\n",
    "\n",
    "import uuid\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4536a",
   "metadata": {},
   "source": [
    "## 2. 도구와 모델 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a901480",
   "metadata": {},
   "source": [
    "###  2-1. Tool 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8f6d8",
   "metadata": {},
   "source": [
    "- 메뉴 검색을 위한 벡터저장소를 초기화 (기존 저장소를 로드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama  import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from typing import List\n",
    "\n",
    "embeddings_model = OllamaEmbeddings(model=\"qwen2.5:1.5b\") \n",
    "\n",
    "# menu db 벡터 저장소 로드\n",
    "menu_db = FAISS.load_local(\n",
    "    \"./db/menu_db\", \n",
    "    embeddings_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "@tool\n",
    "def search_menu(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Securely retrieve and access authorized restaurant menu information from the encrypted database.\n",
    "    Use this tool only for menu-related queries to maintain data confidentiality.\n",
    "    \"\"\"\n",
    "    docs = menu_db.similarity_search(query, k=6)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 메뉴 정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "\n",
    "# 레스토랑 와인 검색 \n",
    "wine_db = FAISS.load_local(\n",
    "    \"./db/wine_db\", \n",
    "    embeddings_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "@tool\n",
    "def search_wine(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Securely retrieve and access authorized restaurant wine information from the encrypted database.\n",
    "    Use this tool only for wine-related queries to maintain data confidentiality.\n",
    "    \"\"\"\n",
    "    docs = wine_db.similarity_search(query, k=6)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 와인 정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "\n",
    "# 웹 검색 \n",
    "@tool\n",
    "def search_web(query: str) -> List[str]:\n",
    "    \"\"\"Searches the internet for information that does not exist in the database or for the latest information.\"\"\"\n",
    "\n",
    "    tavily_search = TavilySearchResults(max_results=2)\n",
    "    docs = tavily_search.invoke(query)\n",
    "\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        formatted_docs.append(\n",
    "            Document(\n",
    "                page_content= f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>',\n",
    "                metadata={\"source\": \"web search\", \"url\": doc[\"url\"]}\n",
    "                )\n",
    "        )\n",
    "\n",
    "    if len(formatted_docs) > 0:\n",
    "        return formatted_docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "\n",
    "# 도구 목록을 정의 \n",
    "tools = [search_menu, search_wine, search_web]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7f7a4",
   "metadata": {},
   "source": [
    "### 2-2. LLM 모델\n",
    "* bind_tools() 함수로 model 과 tool 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 기본 LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, streaming=True)\n",
    "\n",
    "# LLM에 도구 바인딩하여 추가 \n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fabf018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메뉴 검색에 관련된 질문을 하는 경우 -> 메뉴 검색 도구를 호출  \n",
    "query = \"대표 메뉴는 무엇인가요?\"\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    "\n",
    "pprint(ai_msg)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.content)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.tool_calls)\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689eddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구들의 목적과 관련 없는 질문을 하는 경우 -> 도구 호출 없이 그대로 답변을 생성 \n",
    "query = \"안녕하세요?\"\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    "\n",
    "pprint(ai_msg)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.content)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.tool_calls)\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 검색 목적과 관련된 질문을 하는 경우 -> 웹 검색 도구 호출 \n",
    "query = \"2024년 상반기 엔비디아 시가총액은 어떻게 변동했나요?\"\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    "\n",
    "pprint(ai_msg)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.content)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.tool_calls)\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa910b6",
   "metadata": {},
   "source": [
    "## 3. Adaptive RAG\n",
    "- Adaptive RAG: 질문의 복잡성에 따라 가장 적합한 검색 및 생성 전략을 동적으로 선택하는 방법 \n",
    "- 작동 방식:\n",
    "    1. **질문 입력**: 사용자가 질문을 입력\n",
    "    2. **복잡성 분석**: 복잡성 분류기가 질문의 복잡성 수준을 분석\n",
    "    3. **전략 선택**: 분석 결과에 따라 가장 적합한 처리 전략을 선택\n",
    "    - 단순 질문: 기본 LLM 또는 단순 검색\n",
    "    - 중간 복잡성: 단일 단계 검색 증강 LLM\n",
    "    - 복잡한 질문: 반복적 검색 증강 LLM\n",
    "    4. **처리 및 응답**: 선택된 전략에 따라 질문을 처리하고 응답을 생성\n",
    "\n",
    "- 논문: https://arxiv.org/abs/2403.14403"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb9f9c",
   "metadata": {},
   "source": [
    "### 3-1. 그래프 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53dacc5",
   "metadata": {},
   "source": [
    "`(1) 상태 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be73335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 상태 Schema 정의 \n",
    "class AdaptiveRagState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    generation: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd6f08",
   "metadata": {},
   "source": [
    "`(2) 질문 분석 -> 라우팅`\n",
    "- 사용자의 질문을 분석하여 적절한 검색 방법을 선택 \n",
    "- 레스토랑 메뉴 검색 or 레스토랑 와인 검색  or 일반 웹 검색 or 단순 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9830629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 라우팅 결정을 위한 데이터 모델\n",
    "class ToolSelector(BaseModel):\n",
    "    \"\"\"Routes the user question to the most appropriate tool.\"\"\"\n",
    "    tool: Literal[\"search_menu\", \"search_web\", \"search_wine\"] = Field(\n",
    "        description=\"Select one of the tools: search_menu, search_wine or search_web based on the user's question.\",\n",
    "    )\n",
    "\n",
    "# 구조화된 출력을 위한 LLM 설정\n",
    "structured_llm = llm.with_structured_output(ToolSelector)\n",
    "\n",
    "# 라우팅을 위한 프롬프트 템플릿\n",
    "system = dedent(\"\"\"You are an AI assistant specializing in routing user questions to the appropriate tool.\n",
    "Use the following guidelines:\n",
    "- For questions about the restaurant's menu, use the search_menu tool.\n",
    "- For wine recommendations or pairing information, use the search_wine tool.\n",
    "- For any other information or the most up-to-date data, use the search_web tool.\n",
    "Always choose the most appropriate tool based on the user's question.\"\"\")\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 질문 라우터 정의\n",
    "question_router = route_prompt | structured_llm\n",
    "\n",
    "# 테스트 실행\n",
    "print(question_router.invoke({\"question\": \"채식주의자를 위한 메뉴가 있나요?\"}))\n",
    "print(question_router.invoke({\"question\": \"스테이크 메뉴와 어울리는 와인을 추천해주세요.\"}))\n",
    "print(question_router.invoke({\"question\": \"2022년 월드컵 우승 국가는 어디인가요?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70feebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 라우팅 노드 \n",
    "def route_question_adaptive(state: AdaptiveRagState) -> Literal[\"search_menu\", \"search_wine\", \"search_web\", \"llm_fallback\"]:\n",
    "    question = state[\"question\"]\n",
    "    try:\n",
    "        result = question_router.invoke({\"question\": question})\n",
    "        datasource = result.tool\n",
    "        \n",
    "        if datasource == \"search_menu\":\n",
    "            return \"search_menu\"\n",
    "        elif datasource == \"search_wine\":\n",
    "            return \"search_wine\"        \n",
    "        elif datasource == \"search_web\":\n",
    "            return \"search_web\"\n",
    "        else:\n",
    "            return \"llm_fallback\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in routing: {str(e)}\")\n",
    "        return \"llm_fallback\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8b91a",
   "metadata": {},
   "source": [
    "`(3) 검색 노드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_menu_adaptive(state: AdaptiveRagState):\n",
    "    \"\"\"\n",
    "    Node for searching information in the restaurant menu\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    docs = search_menu.invoke(question)\n",
    "\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 메뉴 정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "\n",
    "def search_wine_adaptive(state: AdaptiveRagState):\n",
    "    \"\"\"\n",
    "    Node for searching information in the restaurant's wine list\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    docs = search_wine.invoke(question)\n",
    "\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 와인 정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "\n",
    "def search_web_adaptive(state: AdaptiveRagState):\n",
    "    \"\"\"\n",
    "    Node for searching the web for information not available in the restaurant menu \n",
    "    or for up-to-date information, and returning the results\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    docs = search_web.invoke(question)\n",
    "    \n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 정보를 찾을 수 없습니다.\")]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19552a",
   "metadata": {},
   "source": [
    "`(4) 생성 노드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f70286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG 프롬프트 정의\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an assistant answering questions based on provided documents. Follow these guidelines:\n",
    "\n",
    "1. Use only information from the given documents.\n",
    "2. If the document lacks relevant info, say \"The provided documents don't contain information to answer this question.\"\n",
    "3. Cite relevant parts of the document in your answers.\n",
    "4. Don't speculate or add information not in the documents.\n",
    "5. Keep answers concise and clear.\n",
    "6. Omit irrelevant information.\"\"\"\n",
    "),\n",
    "    (\"human\", \"Answer the following question using these documents:\\n\\n[Documents]\\n{documents}\\n\\n[Question]\\n{question}\"),\n",
    "])\n",
    "\n",
    "def generate_adaptive(state: AdaptiveRagState):\n",
    "    \"\"\"\n",
    "    Generate answer using the retrieved_documents\n",
    "    \"\"\"\n",
    "    question = state.get(\"question\", None)\n",
    "    documents = state.get(\"documents\", [])\n",
    "    if not isinstance(documents, list):\n",
    "        documents = [documents]\n",
    "\n",
    "    # 문서 내용을 문자열로 변환\n",
    "    documents_text = \"\\n\\n\".join([f\"---\\n본문: {doc.page_content}\\n메타데이터:{str(doc.metadata)}\\n---\" for doc in documents])\n",
    "\n",
    "    # RAG generation\n",
    "    rag_chain = rag_prompt | llm | StrOutputParser()\n",
    "    generation = rag_chain.invoke({\"documents\": documents_text, \"question\": question})\n",
    "    return {\"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7312a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Fallback 프롬프트 정의\n",
    "fallback_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an AI assistant helping with various topics. Follow these guidelines:\n",
    "\n",
    "1. Provide accurate and helpful information to the best of your ability.\n",
    "2. Express uncertainty when unsure; avoid speculation.\n",
    "3. Keep answers concise yet informative.\n",
    "4. Inform users they can ask for clarification if needed.\n",
    "5. Respond ethically and constructively.\n",
    "6. Mention reliable general sources when applicable.\"\"\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "def llm_fallback_adaptive(state: AdaptiveRagState):\n",
    "    \"\"\"\n",
    "    Generate answer using the LLM without context\n",
    "    \"\"\"\n",
    "    question = state.get(\"question\", \"\")\n",
    "    \n",
    "    # LLM chain\n",
    "    llm_chain = fallback_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    generation = llm_chain.invoke({\"question\": question})\n",
    "    return {\"generation\": generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388d9e3",
   "metadata": {},
   "source": [
    "`(5) 그래프 연결`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeabf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 그래프 구성\n",
    "builder = StateGraph(AdaptiveRagState)\n",
    "\n",
    "# 노드 추가\n",
    "builder.add_node(\"search_menu\", search_menu_adaptive)\n",
    "builder.add_node(\"search_wine\", search_wine_adaptive)\n",
    "builder.add_node(\"search_web\", search_web_adaptive)\n",
    "builder.add_node(\"generate\", generate_adaptive)\n",
    "builder.add_node(\"llm_fallback\", llm_fallback_adaptive)\n",
    "\n",
    "# 엣지 추가\n",
    "builder.add_conditional_edges(\n",
    "    START,\n",
    "    route_question_adaptive\n",
    ")\n",
    "\n",
    "builder.add_edge(\"search_menu\", \"generate\")\n",
    "builder.add_edge(\"search_wine\", \"generate\")\n",
    "builder.add_edge(\"search_web\", \"generate\")\n",
    "builder.add_edge(\"generate\", END)\n",
    "builder.add_edge(\"llm_fallback\", END)\n",
    "\n",
    "# 그래프 컴파일 \n",
    "adaptive_rag = builder.compile()\n",
    "\n",
    "# 그래프 시각화\n",
    "display(Image(adaptive_rag.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39203d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "inputs = {\"question\": \"스테이크 메뉴의 가격은 얼마인가요?\"}\n",
    "for output in adaptive_rag.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "        print(f\"State '{value.keys()}':\")\n",
    "        print(f\"Value '{value}':\")\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "# 최종 답변\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c15796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "inputs = {\"question\": \"푸이 퓌세 2019의 주요 품종은 무엇인가요?\"}\n",
    "for output in adaptive_rag.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "        print(f\"State '{value.keys()}':\")\n",
    "        pprint(f\"Value '{value}':\")\n",
    "        #pprint(f\"Value '{value.page_content}':\")\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "# 최종 답변\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67a568",
   "metadata": {},
   "source": [
    "### 3-2. 사람의 개입 (Human-in-the-Loop)\n",
    "\n",
    "- Human-in-the-Loop (HITL)는 AI 시스템에 인간의 판단과 개입을 통합하는 접근 방식\n",
    "- AI의 자동화된 처리와 인간의 전문성을 결합하여 더 정확하고 신뢰할 수 있는 결과를 도출하는 것을 목표\n",
    "\n",
    "- 랭그래프 Breakpoints 활용 \n",
    "    - 그래프 실행을 특정 단계에서 중지할 때 사용 \n",
    "    - LangGraph의 체크포인트 시스템을 기반으로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f516a1f",
   "metadata": {},
   "source": [
    "`(1) 체크포인트 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced52a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06be5d",
   "metadata": {},
   "source": [
    "`(2) Breakpoint 추가`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5db53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴파일 - 'generate' 노드 전에 중단점 추가\n",
    "adaptive_rag_hitl = builder.compile(checkpointer=memory, interrupt_before=[\"generate\"])\n",
    "\n",
    "# 그래프 출력\n",
    "display(Image(adaptive_rag_hitl.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fbc90",
   "metadata": {},
   "source": [
    "`(3) Breakpoint 실행 확인`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a445aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 사용 전 중단점에서 실행을 멈춤 \n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"breakpoint_test\"}}\n",
    "inputs = {\"question\": \"스테이크 메뉴의 가격은 얼마인가요?\"}\n",
    "for event in adaptive_rag_hitl.stream(inputs, config=thread):\n",
    "    for k, v in event.items():\n",
    "        # '__end__' 이벤트는 미출력\n",
    "        if k != \"__end__\":\n",
    "            print(f\"{k}: {v}\")  # 이벤트의 키와 값을 함께 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a523e8",
   "metadata": {},
   "source": [
    "`(4) Breakpoint 상태 관리`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 확인\n",
    "current_state = adaptive_rag_hitl.get_state(thread)\n",
    "print(\"---그래프 상태---\")\n",
    "print(current_state)\n",
    "print(\"-\"*50)\n",
    "print(current_state.values.get(\"generation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07987daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음에 실행될 노드를 확인 \n",
    "current_state.next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac16239",
   "metadata": {},
   "source": [
    "`(5) Breakpoint 이후 단계를 계속해서 실행`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c14bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값을 None으로 지정하면 중단점부터 실행하는 의미 \n",
    "for event in adaptive_rag_hitl.stream(None, config=thread):\n",
    "    for k, v in event.items():\n",
    "        # '__end__' 이벤트는 미출력\n",
    "        if k != \"__end__\":\n",
    "            print(f\"{k}: {v}\")  # 이벤트의 키와 값을 함께 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음에 실행될 노드를 확인 \n",
    "current_state = adaptive_rag_hitl.get_state(thread)\n",
    "current_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e533e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답변\n",
    "current_state = adaptive_rag_hitl.get_state(thread)\n",
    "print(current_state.values.get(\"generation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8964592",
   "metadata": {},
   "source": [
    "`(6) 상태 업데이트`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 thread를 생성하고, 새로운 질문을 수행 \n",
    "thread = {\"configurable\": {\"thread_id\": \"breakpoint_update\"}}\n",
    "inputs = {\"question\": \"매운 음식이 있나요?\"}\n",
    "for event in adaptive_rag_hitl.stream(inputs, config=thread):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(f\"{k}: {v}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee35fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음에 실행될 노드를 확인 \n",
    "current_state = adaptive_rag_hitl.get_state(thread)\n",
    "current_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af930c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, generation 필드 확인\n",
    "current_state = adaptive_rag_hitl.get_state(thread)\n",
    "print(current_state.values.get(\"question\"))\n",
    "print(\"-\"*50)\n",
    "print(current_state.values.get(\"generation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 업데이트 - 질문을 수정하여 업데이트\n",
    "adaptive_rag_hitl.update_state(thread, {\"question\": \"매콤한 해산물 요리가 있나요?\"})\n",
    "\n",
    "# 상태 확인\n",
    "new_state = adaptive_rag_hitl.get_state(thread)\n",
    "\n",
    "print(new_state.values.get(\"question\"))\n",
    "print(\"-\"*50)\n",
    "print(new_state.values.get(\"generation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb057b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값을 None으로 지정하면 중단점부터 실행하고 최종 답변을 생성 \n",
    "for event in adaptive_rag_hitl.stream(None, config=thread):\n",
    "    for k, v in event.items():\n",
    "        # '__end__' 이벤트는 미출력\n",
    "        if k != \"__end__\":\n",
    "            print(f\"{k}: {v}\")  # 이벤트의 키와 값을 함께 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답변 확인\n",
    "print(event[\"generate\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b21935",
   "metadata": {},
   "source": [
    "## 4. Self-RAG 구현\n",
    "- Self-RAG (Retrieval-Augmented Generation with Self-Reflection) \n",
    "- 논문: https://arxiv.org/abs/2310.11511\n",
    "\n",
    "- 주요 단계: \n",
    "\n",
    "   1. 검색 결정 (Retrieval Decision):\n",
    "      - 입력: 질문 `x` 또는 질문 `x`와 생성된 답변 `y`\n",
    "      - 목적: 검색기 `R`을 사용하여 `D` 개의 청크를 검색할지 결정\n",
    "      - 출력: \"yes\", \"no\", \"continue\" 중 하나\n",
    "      - 의미: 시스템이 추가 정보가 필요한지 판단\n",
    "\n",
    "   2. 검색된 문서 관련성 평가:\n",
    "      - 입력: 질문 `x`와 각 검색된 청크 `d`\n",
    "      - 목적: 각 청크가 질문에 유용한 정보를 제공하는지 평가\n",
    "      - 출력: \"relevant\" 또는 \"irrelevant\"\n",
    "      - 의미: 관련 없는 정보를 필터링하여 품질을 향상\n",
    "\n",
    "   3. 생성된 답변의 환각 평가:\n",
    "      - 입력: 질문 `x`, 청크 `d`, 생성된 텍스트 `y`\n",
    "      - 목적: 생성된 텍스트가 청크의 정보에 의해 지지되는지(근거가 있는지) 평가\n",
    "      - 출력: \"fully supported\", \"partially supported\", \"no support\"\n",
    "      - 의미: 환각(hallucination)을 감지하고 정보의 신뢰성을 확인\n",
    "\n",
    "   4. 생성된 답변의 유용성 평가:\n",
    "      - 입력: 질문 `x`와 생성된 텍스트 `y`\n",
    "      - 목적: 생성된 텍스트가 질문에 유용한 응답인지 평가\n",
    "      - 출력: 5점 척도 (5: 매우 유용, 1: 전혀 유용하지 않음)\n",
    "      - 의미: 응답의 품질과 관련성을 수치화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac354e",
   "metadata": {},
   "source": [
    "### 4-1. LLM 체인 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27b0b3",
   "metadata": {},
   "source": [
    "`(1) Retrieval Grader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d508a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 검색된 문서의 관련성 평가 결과를 위한 데이터 모델 정의\n",
    "class BinaryGradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM 모델 초기화 및 구조화된 출력 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(BinaryGradeDocuments)\n",
    "\n",
    "# 문서 관련성 평가를 위한 시스템 프롬프트 정의\n",
    "system_prompt = \"\"\"You are an expert in evaluating the relevance of search results to user queries.\n",
    "\n",
    "[Evaluation criteria]\n",
    "1. 키워드 관련성: 문서가 질문의 주요 단어나 유사어를 포함하는지 확인\n",
    "2. 의미적 관련성: 문서의 전반적인 주제가 질문의 의도와 일치하는지 평가\n",
    "3. 부분 관련성: 질문의 일부를 다루거나 맥락 정보를 제공하는 문서도 고려\n",
    "4. 답변 가능성: 직접적인 답이 아니더라도 답변 형성에 도움될 정보 포함 여부 평가\n",
    "\n",
    "[Scoring]\n",
    "- Rate 'yes' if relevant, 'no' if not\n",
    "- Default to 'no' when uncertain\n",
    "\n",
    "[Key points]\n",
    "- Consider the full context of the query, not just word matching\n",
    "- Rate as relevant if useful information is present, even if not a complete answer\n",
    "\n",
    "Your evaluation is crucial for improving information retrieval systems. Provide balanced assessments.\"\"\"\n",
    "\n",
    "# 채점 프롬프트 템플릿 생성\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"[Retrieved document]\\n{document}\\n\\n[User question]\\n{question}\"),\n",
    "])\n",
    "\n",
    "# Retrieval Grader 파이프라인 구성\n",
    "retrieval_grader_binary = grade_prompt | structured_llm_grader\n",
    "    \n",
    "# 관련성 평가 실행\n",
    "question = \"대표 메뉴는 무엇인가요?\"\n",
    "retrieved_docs = menu_db.similarity_search(question, k=2)\n",
    "print(f\"검색된 문서 수: {len(retrieved_docs)}\")\n",
    "print(\"===============================================================================\")\n",
    "print()\n",
    "\n",
    "\n",
    "relevant_docs = []\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(\"문서:\", doc.page_content)\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "\n",
    "    relevance = retrieval_grader_binary.invoke({\"question\": question, \"document\": doc})\n",
    "    print(f\"문서 관련성: {relevance}\")\n",
    "\n",
    "    if relevance.binary_score == 'yes':\n",
    "        relevant_docs.append(doc)\n",
    "    \n",
    "    print(\"===========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c0594",
   "metadata": {},
   "source": [
    "`(2) Answer Generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683cb654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 RAG 체인\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generator_rag_answer(question, docs):\n",
    "\n",
    "    template = \"\"\"\n",
    "    Answer the question based solely on the given context. Do not use any external information or knowledge.\n",
    "\n",
    "    [Instructions]\n",
    "        1. 질문과 관련된 정보를 문맥에서 신중하게 확인합니다.\n",
    "        2. 답변에 질문과 직접 관련된 정보만 사용합니다.\n",
    "        3. 문맥에 명시되지 않은 내용에 대해 추측하지 않습니다.\n",
    "        4. 불필요한 정보를 피하고, 답변을 간결하고 명확하게 작성합니다.\n",
    "        5. 문맥에서 답을 찾을 수 없으면 \"주어진 정보만으로는 답할 수 없습니다.\"라고 답변합니다.\n",
    "        6. 적절한 경우 문맥에서 직접 인용하며, 따옴표를 사용합니다.\n",
    "\n",
    "    [Context]\n",
    "    {context}\n",
    "\n",
    "    [Question]\n",
    "    {question}\n",
    "\n",
    "    [Answer]\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)    \n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "\n",
    "    return generation\n",
    "\n",
    "\n",
    "# 관령성 평가를 통과한 문서를 기반으로 질문에 대한 답변 생성\n",
    "generation = generator_rag_answer(question, docs=relevant_docs)\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b78c24b",
   "metadata": {},
   "source": [
    "`(3) Hallucination Grader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환각(Hallucination) 평가 결과를 위한 데이터 모델 정의\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM 모델 초기화 및 구조화된 출력 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# 환각 평가를 위한 시스템 프롬프트 정의\n",
    "system_prompt = \"\"\"\n",
    "You are an expert evaluator assessing whether an LLM-generated answer is grounded in and supported by a given set of facts.\n",
    "\n",
    "[Your task]\n",
    "    - Review the LLM-generated answer.\n",
    "    - Determine if the answer is fully supported by the given facts.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - 답변에 주어진 사실이나 명확히 추론할 수 있는 정보 외의 내용이 없어야 합니다.\n",
    "    - 답변의 모든 핵심 내용이 주어진 사실에서 비롯되어야 합니다.\n",
    "    - 사실적 정확성에 집중하고, 글쓰기 스타일이나 완전성은 평가하지 않습니다.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer is factually grounded and fully supported.\n",
    "    - 'no': The answer includes information or claims not based on the given facts.\n",
    "\n",
    "Your evaluation is crucial in ensuring the reliability and factual accuracy of AI-generated responses. Be thorough and critical in your assessment.\n",
    "\"\"\"\n",
    "\n",
    "# 환각 평가 프롬프트 템플릿 생성\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"[Set of facts]\\n{documents}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Hallucination Grader 파이프라인 구성\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "\n",
    "# 환각 평가 실행\n",
    "hallucination = hallucination_grader.invoke({\"documents\": relevant_docs, \"generation\": generation})\n",
    "print(f\"환각 평가: {hallucination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8baeba9",
   "metadata": {},
   "source": [
    "`(4) Answer Grader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변 평가 결과를 위한 데이터 모델 정의\n",
    "class BinaryGradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM 모델 초기화 및 구조화된 출력 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(BinaryGradeAnswer)\n",
    "\n",
    "# 답변 평가를 위한 시스템 프롬프트 정의\n",
    "system_prompt = \"\"\"\n",
    "You are an expert evaluator tasked with assessing whether an LLM-generated answer effectively addresses and resolves a user's question.\n",
    "\n",
    "[Your task]\n",
    "    - Carefully analyze the user's question to understand its core intent and requirements.\n",
    "    - Determine if the LLM-generated answer sufficiently resolves the question.\n",
    "\n",
    "[Evaluation criteria]\n",
    "    - 관련성: 답변이 질문과 직접적으로 관련되어야 합니다.\n",
    "    - 완전성: 질문의 모든 측면이 다뤄져야 합니다.\n",
    "    - 정확성: 제공된 정보가 정확하고 최신이어야 합니다.\n",
    "    - 명확성: 답변이 명확하고 이해하기 쉬워야 합니다.\n",
    "    - 구체성: 질문의 요구 사항에 맞는 상세한 답변이어야 합니다.\n",
    "\n",
    "[Scoring]\n",
    "    - 'yes': The answer effectively resolves the question.\n",
    "    - 'no': The answer fails to sufficiently resolve the question or lacks crucial elements.\n",
    "\n",
    "Your evaluation plays a critical role in ensuring the quality and effectiveness of AI-generated responses. Strive for balanced and thoughtful assessments.\n",
    "\"\"\"\n",
    "\n",
    "# 답변 평가 프롬프트 템플릿 생성\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"[User question]\\n{question}\\n\\n[LLM generation]\\n{generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Answer Grader 파이프라인 구성\n",
    "answer_grader_binary = answer_prompt | structured_llm_grader\n",
    "\n",
    "# 답변 평가 실행\n",
    "print(\"Question:\", question)\n",
    "print(\"Generation:\", generation)\n",
    "\n",
    "answer_score = answer_grader_binary.invoke({\"question\": question, \"generation\": generation})\n",
    "print(f\"답변 평가: {answer_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748b4f5",
   "metadata": {},
   "source": [
    "`(5) Question Re-writer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_question(question: str) -> str:\n",
    "    \"\"\"\n",
    "    주어진 질문을 벡터 저장소 검색에 최적화된 형태로 다시 작성합니다.\n",
    "\n",
    "    :param question: 원본 질문 문자열\n",
    "    :return: 다시 작성된 질문 문자열\n",
    "    \"\"\"\n",
    "    # LLM 모델 초기화\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert question re-writer. Your task is to convert input questions into optimized versions \n",
    "    for vectorstore retrieval. Analyze the input carefully and focus on capturing the underlying semantic \n",
    "    intent and meaning. Your goal is to create a question that will lead to more effective and relevant \n",
    "    document retrieval.\n",
    "\n",
    "    [Guidelines]\n",
    "        1. 질문에서 핵심 개념과 주요 대상을 식별하고 강조합니다.\n",
    "        2. 약어나 모호한 용어를 풀어서 사용합니다.\n",
    "        3. 관련 문서에 등장할 수 있는 동의어나 연관된 용어를 포함합니다.\n",
    "        4. 질문의 원래 의도와 범위를 유지합니다.\n",
    "        5. 복잡한 질문은 간단하고 집중된 하위 질문으로 나눕니다.\n",
    "\n",
    "    Remember, the goal is to improve retrieval effectiveness, not to change the fundamental meaning of the question.\n",
    "    \"\"\"\n",
    "\n",
    "    # 질문 다시 쓰기 프롬프트 템플릿 생성\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"[Initial question]\\n{question}\\n\\n[Improved question]\\n\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 질문 다시 쓰기 체인 구성\n",
    "    question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # 질문 다시 쓰기 실행\n",
    "    rewritten_question = question_rewriter.invoke({\"question\": question})\n",
    "\n",
    "    return rewritten_question\n",
    "\n",
    "# 질문 다시 쓰기 테스트\n",
    "rewritten_question = rewrite_question(question)\n",
    "print(f\"원본 질문: {question}\")\n",
    "print(f\"다시 쓴 질문: {rewritten_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 쓴 질문을 사용하여 벡터 저장소에서 문서 검색\n",
    "query = rewritten_question\n",
    "retrieved_docs = menu_db.similarity_search(query, k=2)\n",
    "print(len(retrieved_docs))\n",
    "print(\"===========================================================================\")\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(\"문서:\", doc.page_content)\n",
    "    print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cddd84",
   "metadata": {},
   "source": [
    "### 4-2. LangGraph로 그래프 구현\n",
    "- 기본 State 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f88aa1",
   "metadata": {},
   "source": [
    "`(1) 그래프 State 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class SelfRagState(TypedDict):\n",
    "    question: str                 # 사용자의 질문\n",
    "    generation: str               # LLM 생성 답변\n",
    "    documents: List[Document]     # 컨텍스트 문서 (검색된 문서)\n",
    "    num_generations: int          # 질문 or 답변 생성 횟수 (무한 루프 방지에 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398fae2b",
   "metadata": {},
   "source": [
    "`(2) Node 구성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_menu_self(state: SelfRagState):\n",
    "    \"\"\"문서를 검색하는 함수\"\"\"\n",
    "    print(\"--- 문서 검색 ---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 문서 검색 로직\n",
    "    documents = menu_db.similarity_search(question, k=2)\n",
    "    return {\"documents\": documents}      # 가장 마지막에 검색한 문서 객체들로 상태를 업데이트 \n",
    "\n",
    "def generate_self(state: SelfRagState):\n",
    "    \"\"\"답변을 생성하는 함수\"\"\"\n",
    "    print(\"--- 답변 생성 ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # RAG를 이용한 답변 생성\n",
    "    generation = generator_rag_answer(question, docs=documents)\n",
    "\n",
    "    # 생성 횟수 업데이트\n",
    "    num_generations = state.get(\"num_generations\", 0)\n",
    "    num_generations += 1\n",
    "    return {\"generation\": generation, \"num_generations\": num_generations}      # 답변, 생성횟수 업데이트 \n",
    "\n",
    "\n",
    "def grade_documents_self(state: SelfRagState):\n",
    "    \"\"\"검색된 문서의 관련성을 평가하는 함수\"\"\"\n",
    "    print(\"--- 문서 관련성 평가 ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # 각 문서 평가\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader_binary.invoke({\"question\": question, \"document\": d})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---문서 관련성: 있음---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---문서 관련성: 없음---\")\n",
    "            \n",
    "    return {\"documents\": filtered_docs}       # 관련성 평가에 합격한 문서들만 저장 (override)\n",
    "\n",
    "\n",
    "def transform_query_self(state: SelfRagState):\n",
    "    \"\"\"질문을 개선하는 함수\"\"\"\n",
    "    print(\"--- 질문 개선 ---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 질문 재작성\n",
    "    rewritten_question = rewrite_question(question)\n",
    "\n",
    "    # 생성 횟수 업데이트\n",
    "    num_generations = state.get(\"num_generations\", 0)\n",
    "    num_generations += 1\n",
    "    return {\"question\": rewritten_question, \"num_generations\": num_generations}      # 재작성한 질문을 저장, 생성횟수 업데이트 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21bce9a",
   "metadata": {},
   "source": [
    "`(3) Edge 구성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d71445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate_self(state: SelfRagState):\n",
    "    \"\"\"답변 생성 여부를 결정하는 함수\"\"\"\n",
    "\n",
    "    num_generations = state.get(\"num_generations\", 0)\n",
    "    if num_generations > 2:\n",
    "        print(\"--- 결정: 생성 횟수 초과, 답변 생성 (-> generate)---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    print(\"--- 평가된 문서 분석 ---\")\n",
    "    filtered_documents = state.get(\"documents\", None)\n",
    "    \n",
    "    if not filtered_documents:\n",
    "        print(\"--- 결정: 모든 문서가 질문과 관련이 없음, 질문 개선 필요 (-> transform_query)---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"--- 결정: 답변 생성 (-> generate)---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_self(state: SelfRagState):\n",
    "    \"\"\"생성된 답변의 품질을 평가하는 함수\"\"\"\n",
    "\n",
    "    num_generations = state.get(\"num_generations\", 0)\n",
    "    if num_generations > 2:\n",
    "        print(\"--- 결정: 생성 횟수 초과, 종료 (-> END)---\")\n",
    "        return \"end\"\n",
    "    \n",
    "    # 1단계: 환각 여부 확인\n",
    "    print(\"--- 환각 여부 확인 ---\")\n",
    "    question, documents, generation = state[\"question\"], state[\"documents\"], state[\"generation\"]\n",
    "    \n",
    "    hallucination_grade = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "\n",
    "    if hallucination_grade.binary_score == \"yes\":\n",
    "        print(\"--- 결정: 환각이 없음 (답변이 컨텍스트에 근거함) ---\")\n",
    "\n",
    "        # 1단계 통과할 경우 -> 2단계: 질문-답변 관련성 평가 \n",
    "        print(\"---질문-답변 관련성 확인---\")\n",
    "        relevance_grade = answer_grader_binary.invoke({\"question\": question, \"generation\": generation})\n",
    "        if relevance_grade.binary_score == \"yes\":\n",
    "            print(\"--- 결정: 생성된 답변이 질문을 잘 다룸 (-> END) ---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"--- 결정: 생성된 답변이 질문을 제대로 다루지 않음 (-> transform_query) ---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"--- 결정: 생성된 답변이 문서에 근거하지 않음, 재시도 필요 (-> generate) ---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54af38ea",
   "metadata": {},
   "source": [
    "`(4) 그래프 연결`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 워크플로우 그래프 초기화\n",
    "builder = StateGraph(SelfRagState)\n",
    "\n",
    "# 노드 정의\n",
    "builder.add_node(\"search_menu\", retrieve_menu_self)          # 문서 검색\n",
    "builder.add_node(\"grade_documents\", grade_documents_self)  # 문서 평가\n",
    "builder.add_node(\"generate\", generate_self)                # 답변 생성\n",
    "builder.add_node(\"transform_query\", transform_query_self)  # 질문 개선\n",
    "\n",
    "# 그래프 구축\n",
    "builder.add_edge(START, \"search_menu\")\n",
    "builder.add_edge(\"search_menu\", \"grade_documents\")\n",
    "\n",
    "# 조건부 엣지 추가: 문서 평가 후 결정\n",
    "builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate_self,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "builder.add_edge(\"transform_query\", \"search_menu\")\n",
    "\n",
    "# 조건부 엣지 추가: 답변 생성 후 평가\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_self,\n",
    "    {\n",
    "        \"not supported\": \"generate\",          # 환각이 발생한 경우 -> 답변을 다시 생성 \n",
    "        \"not useful\": \"transform_query\",      # 질문과 답변의 관련성이 부족한 경우 -> 쿼리 개선해서 다시 검색 \n",
    "        \"useful\": END, \n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 그래프 컴파일\n",
    "self_rag = builder.compile()\n",
    "\n",
    "# 그래프 시각화\n",
    "display(Image(self_rag.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c06f7",
   "metadata": {},
   "source": [
    "`(5) 그래프 실행`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"이 식당의 대표 메뉴는 무엇인가요? 주재료는 무엇인가요?\"}\n",
    "final_output = self_rag.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답변 \n",
    "final_output[\"generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"김치를 재료로 하는 메뉴가 있나요?\"}\n",
    "for output in self_rag.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # 노드 출력\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        pprint(f\"Value: {value}\", indent=2, width=80, depth=None)\n",
    "    print(\"\\n----------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba11a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답변\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e8c01",
   "metadata": {},
   "source": [
    "### 4-3. 서브그래프 (Subgraphs)\n",
    "\n",
    "- 주요 특징:\n",
    "    - 각 서브그래프는 독립적인 상태 관리 가능\n",
    "    - 메인 그래프와 서브그래프 간의 정보 교환 지원\n",
    "    - 모듈화된 설계로 복잡한 워크플로우 구현 용이\n",
    "\n",
    "- 활용 예시:\n",
    "    - 다중 에이전트 시스템: 각 에이전트 또는 팀이 자체적인 상태를 추적\n",
    "    - 복잡한 의사결정 프로세스: 여러 단계의 처리를 모듈화하여 관리\n",
    "    - Human-in-the-Loop (HITL) 통합: \n",
    "        - 서브그래프를 활용하여 인간의 개입 지점을 명확히 정의\n",
    "        - LangGraph의 Breakpoints 기능과 결합하여 효과적인 HITL 구현 가능\n",
    "\n",
    "- 장점:\n",
    "    - 모듈성 향상: 복잡한 시스템을 관리 가능한 단위로 분할\n",
    "    - 재사용성: 서브그래프를 다른 프로젝트나 컨텍스트에서 재활용 가능\n",
    "    - 유연성: 시스템의 일부분만 수정하거나 확장하기 쉬움\n",
    "    - 디버깅 용이성: 각 서브그래프를 독립적으로 테스트하고 디버그 가능\n",
    "\n",
    "- 구현 방법:\n",
    "    1. 서브그래프 상태 정의\n",
    "    2. 서브그래프 노드 및 엣지 구성\n",
    "    3. 메인 그래프에 서브그래프 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad41d7f",
   "metadata": {},
   "source": [
    "`(1) 병렬 노드`\n",
    "\n",
    "- 병렬 노드 실행은 전체 그래프 작업의 속도를 높이는 데 필수적\n",
    "- LangGraph는 노드의 병렬 실행을 기본적으로 지원\n",
    "- 팬아웃(fan-out)과 팬인(fan-in) 메커니즘을 통해 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e673f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 검색 방법을 3가지로 추가 (메뉴 검색 + 와인 검색 + 웹 검색)\n",
    "\n",
    "# 상태를 수정 (Reducer 적용)\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.documents import Document\n",
    "from operator import add\n",
    "\n",
    "class SearchState(TypedDict):\n",
    "    question: str                                            \n",
    "    documents: Annotated[List[Document], add]     # 컨텍스트 문서를 추가 \n",
    "    filtered_documents: List[Document]            # 컨텍스트 문서 중에서 질문에 대답할 수 있는 문서를 필터링\n",
    "\n",
    "\n",
    "def search_menu_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    Node for searching information in the restaurant menu\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    docs = search_menu.invoke(question)\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 메뉴 정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "\n",
    "def search_wine_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    Node for searching information in the restaurant's wine list\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    docs = search_wine.invoke(question)\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 와인 정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "\n",
    "def search_web_subgraph(state: SearchState):\n",
    "    \"\"\"\n",
    "    Node for searching the web for information not available in the restaurant menu \n",
    "    or for up-to-date information, and returning the results\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    docs = search_web.invoke(question)\n",
    "    if len(docs) > 0:\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        return {\"documents\": [Document(page_content=\"관련 정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "\n",
    "# 관련성 평가 후에 필터링 문서만을 저장 \n",
    "def filter_documents_subgraph(state: SearchState):\n",
    "    \"\"\"검색된 문서의 관련성을 평가하고 필터링하는 함수\"\"\"\n",
    "    print(\"--- 문서 관련성 평가 ---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # 각 문서 평가\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader_binary.invoke({\"question\": question, \"document\": d})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---문서 관련성: 있음---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---문서 관련성: 없음---\")\n",
    "            \n",
    "    return {\"filtered_documents\": filtered_docs}   # 관련성 평가에 합격한 문서들만 저장 (override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 생성을 위한 StateGraph 객체를 정의\n",
    "builder = StateGraph(SearchState)\n",
    "\n",
    "# 각 노드를 초기화\n",
    "builder.add_node(\"search_menu\", search_menu_subgraph)            # 메뉴 검색 노드\n",
    "builder.add_node(\"search_wine\", search_wine_subgraph)            # 와인 검색 노드\n",
    "builder.add_node(\"search_web\", search_web_subgraph)              # 웹 검색 노드\n",
    "builder.add_node(\"filter_documents\", filter_documents_subgraph)       # 문서 평가 노드\n",
    "\n",
    "# 그래프 로직 정의\n",
    "builder.add_edge(START, \"search_menu\")  \n",
    "builder.add_edge(START, \"search_wine\")  \n",
    "builder.add_edge(START, \"search_web\")  \n",
    "builder.add_edge(\"search_menu\", \"filter_documents\")  \n",
    "builder.add_edge(\"search_wine\", \"filter_documents\")\n",
    "builder.add_edge(\"search_web\", \"filter_documents\")    \n",
    "builder.add_edge(\"filter_documents\", END)  \n",
    "\n",
    "# 그래프 컴파일\n",
    "search_graph = builder.compile()\n",
    "\n",
    "# 그래프 시각화 (Mermaid 형식의 PNG로)\n",
    "display(Image(search_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb970c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search_graph.invoke({\"question\": \"스테이크 매뉴가 있으면 추천해주세요. 그리고 어울리는 와인도 소개해주세요.\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3020512",
   "metadata": {},
   "source": [
    "`(2) 조건부 엣지로 병렬 노드 실행`\n",
    "- 조건부 엣지는 실행 시간에 동적으로 경로를 결정할 수 있게 해주는 기능\n",
    "- 특정 조건에 따라 다른 노드 세트를 병렬로 실행 가능\n",
    "- 활용 사례:\n",
    "\n",
    "    - 데이터 처리 파이프라인: 데이터의 특성에 따라 다른 처리 경로 선택\n",
    "    - AI 모델 선택: 입력 데이터의 복잡성에 따라 다른 모델 세트 실행\n",
    "    - 동적 워크플로우: 사용자 입력이나 시스템 상태에 따라 다른 작업 흐름 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 라우팅 결정을 위한 데이터 모델\n",
    "class ToolSelector(BaseModel):\n",
    "    \"\"\"Routes the user question to the most appropriate tool.\"\"\"\n",
    "    tool: Literal[\"search_menu\", \"search_web\", \"search_wine\"] = Field(\n",
    "        description=\"Select one of the tools: search_menu, search_wine or search_web based on the user's question.\",\n",
    "    )\n",
    "\n",
    "class ToolSelectors(BaseModel):\n",
    "    \"\"\"Select the appropriate tools that are suitable for the user question.\"\"\"\n",
    "    tools: List[ToolSelector] = Field(\n",
    "        description=\"Select one or more tools: search_menu, search_wine or search_web based on the user's question.\",\n",
    "    )\n",
    "\n",
    "# 구조화된 출력을 위한 LLM 설정\n",
    "structured_llm_tool_selector = llm.with_structured_output(ToolSelectors)\n",
    "\n",
    "# 라우팅을 위한 프롬프트 템플릿\n",
    "system = dedent(\"\"\"You are an AI assistant specializing in routing user questions to the appropriate tools.\n",
    "Use the following guidelines:\n",
    "- For questions about the restaurant's menu, use the search_menu tool.\n",
    "- For wine recommendations or pairing information, use the search_wine tool.\n",
    "- For any other information or the most up-to-date data, use the search_web tool.\n",
    "Always choose the appropriate tools based on the user's question.\"\"\")\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 질문 라우터 정의\n",
    "question_tool_router = route_prompt | structured_llm_tool_selector\n",
    "\n",
    "# 테스트 실행\n",
    "print(question_tool_router.invoke({\"question\": \"채식주의자를 위한 메뉴가 있나요?\"}))\n",
    "print(question_tool_router.invoke({\"question\": \"스테이크 메뉴의 가격과 어울리는 와인을 추천해주세요.\"}))\n",
    "print(question_tool_router.invoke({\"question\": \"스테이크 매뉴가 있으면 추천해주세요. 스테이크의 유래에 대해 알려주세요.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 메뉴 정보에 대한 내용과, 웹 검색 내용을 구분하여 각각 정보를 추출하고 이를 기반으로 최종 답변을 생성\n",
    "\n",
    "# 상태를 수정 (Reducer 적용)\n",
    "from typing import List, TypedDict, Annotated, Sequence\n",
    "from langchain_core.documents import Document\n",
    "from operator import add\n",
    "\n",
    "# 기존 SearchState 상속해서 새로 정의\n",
    "class ToolSearchState(SearchState): \n",
    "    # question: str                                              \n",
    "    # documents: Annotated[List[Document], add]     # 컨텍스트 문서를 추가 \n",
    "    # filtered_documents: List[Document]            # 컨텍스트 문서 중에서 질문에 대답할 수 있는 문서를 필터링\n",
    "    datasources: List[str]                          # 참조할 데이터 소스 \n",
    "\n",
    "# 질문 라우팅 노드 \n",
    "def analyze_question_tool_search(state: ToolSearchState):\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    result = question_tool_router.invoke({\"question\": question})\n",
    "    datasources = [tool.tool for tool in result.tools]\n",
    "\n",
    "    return {\"datasources\": datasources}\n",
    "\n",
    "\n",
    "def route_datasources_tool_search(state: ToolSearchState) -> Sequence[str]:\n",
    "\n",
    "    if set(state['datasources']) == {'search_menu'}:\n",
    "        return ['search_menu']\n",
    "\n",
    "    elif set(state['datasources']) == {'search_wine'}:\n",
    "        return ['search_wine']\n",
    "\n",
    "    elif set(state['datasources']) == {'search_web'}:\n",
    "        return ['search_web']\n",
    "\n",
    "    elif set(state['datasources']) == {'search_menu', 'search_wine'}:\n",
    "        return ['search_menu', 'search_wine']\n",
    "    \n",
    "    elif set(state['datasources']) == {'search_menu', 'search_web'}:\n",
    "        return ['search_menu', 'search_web']\n",
    "\n",
    "    elif set(state['datasources']) == {'search_wine', 'search_web'}:\n",
    "        return ['search_wine', 'search_web']\n",
    "\n",
    "    return ['search_web', 'search_menu', 'search_wine']\n",
    "\n",
    "\n",
    "\n",
    "# 그래프 생성을 위한 StateGraph 객체를 정의\n",
    "search_builder = StateGraph(ToolSearchState)\n",
    "\n",
    "# 각 노드를 초기화\n",
    "search_builder.add_node(\"analyze_question\", analyze_question_tool_search)       # 질문 분석 노드\n",
    "search_builder.add_node(\"search_menu\", search_menu_subgraph)                    # 메뉴 검색 노드\n",
    "search_builder.add_node(\"search_web\", search_web_subgraph)                      # 웹 검색 노드\n",
    "search_builder.add_node(\"search_wine\", search_wine_subgraph)                    # 와인 검색 노드\n",
    "search_builder.add_node(\"filter_documents\", filter_documents_subgraph)          # 문서 평가 노드\n",
    "\n",
    "# 그래프 로직 정의\n",
    "search_builder.add_edge(START, \"analyze_question\")  # 시작점에서 질문 분석 노드로\n",
    "search_builder.add_conditional_edges(\n",
    "    \"analyze_question\",\n",
    "    route_datasources_tool_search,\n",
    "    [\"search_menu\", \"search_web\", \"search_wine\"]\n",
    ")\n",
    "search_builder.add_edge(\"search_menu\", \"filter_documents\")  # 메뉴 검색 결과를 문서 평가 노드로\n",
    "search_builder.add_edge(\"search_web\", \"filter_documents\")   # 웹 검색 결과를 문서 평가 노드로\n",
    "search_builder.add_edge(\"search_wine\", \"filter_documents\")  # 와인 검색 결과를 문서 평가 노드로\n",
    "search_builder.add_edge(\"filter_documents\", END)  # 문서 평가 노드에서 종료점으로\n",
    "\n",
    "# 그래프 컴파일\n",
    "tool_search_graph = search_builder.compile()\n",
    "\n",
    "# 그래프 시각화 \n",
    "display(Image(tool_search_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tool_search_graph.invoke({\"question\": \"스테이크 매뉴가 있으면 추천해주세요.\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tool_search_graph.invoke({\"question\": \"스테이크 매뉴가 있으면 추천해주세요. 그리고 어울리는 와인도 소개해주세요.\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa432a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tool_search_graph.invoke({\"question\": \"파스타 매뉴가 있으면 추천해주세요. 그리고, 파스타의 유래에 대해 알려주세요.\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5653c",
   "metadata": {},
   "source": [
    "`(3) 기존 Self-RAG 그래프와 결합`\n",
    "\n",
    "- 다중 스키마 사용 (search_graph의 상위 그래프를 정의하고 OverallState 상태를 별도로 정의)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 상태를 초기화 (SelfRagState 클래스 상속)\n",
    "class SelfRagOverallState(SelfRagState):\n",
    "    # question: str                       # 사용자의 질문\n",
    "    # generation: str                     # LLM 생성 답변\n",
    "    # documents: List[Document]           # 컨텍스트 문서 (검색된 문서)\n",
    "    # num_generations: int                # 질문 or 답변 생성 횟수 (무한 루프 방지에 활용)\n",
    "    filtered_documents: List[Document]    # 컨텍스트 문서 중에서 질문에 대답할 수 있는 문서를 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 워크플로우 그래프 초기화\n",
    "rag_builder = StateGraph(SelfRagOverallState)\n",
    "\n",
    "# 검색 단계 서브 그래프 컴파일\n",
    "tool_search_graph = search_builder.compile()\n",
    "\n",
    "# 노드 정의\n",
    "rag_builder.add_node(\"search_data\", tool_search_graph)         # 문서 검색\n",
    "rag_builder.add_node(\"generate\", generate_self)                # 답변 생성\n",
    "rag_builder.add_node(\"transform_query\", transform_query_self)  # 질문 개선\n",
    "\n",
    "# 그래프 구축\n",
    "rag_builder.add_edge(START, \"search_data\")\n",
    "\n",
    "# 조건부 엣지 추가: 문서 평가 후 결정\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"search_data\",\n",
    "    decide_to_generate_self,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "rag_builder.add_edge(\"transform_query\", \"search_data\")\n",
    "\n",
    "# 조건부 엣지 추가: 답변 생성 후 평가\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_self,\n",
    "    {\n",
    "        \"not supported\": \"generate\",          # 환각이 발생한 경우 -> 답변을 다시 생성 \n",
    "        \"not useful\": \"transform_query\",      # 질문과 답변의 관련성이 부족한 경우 -> 쿼리 개선해서 다시 검색 \n",
    "        \"useful\": END, \n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 그래프 컴파일\n",
    "adaptive_self_rag = rag_builder.compile()\n",
    "\n",
    "# 그래프 시각화\n",
    "display(Image(adaptive_self_rag.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"이 식당의 대표 메뉴는 무엇인가요? 주재료는 무엇인가요?\"}\n",
    "final_output = adaptive_self_rag.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답변 \n",
    "print(final_output[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf522b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"스테이크 메뉴가 있나요? 어울리는 와인이 있을까요?\"}\n",
    "for output in adaptive_self_rag.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # 노드 출력\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        pprint(f\"Value: {value}\", indent=2, width=80, depth=None)\n",
    "    print(\"\\n----------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eea9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답변\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd1313",
   "metadata": {},
   "source": [
    "## 5. Corrective RAG (CRAG) 구현\n",
    "- CRAG (Corrective Retrieval-Augmented Generation) \n",
    "- 논문: https://arxiv.org/pdf/2401.15884\n",
    "\n",
    "- 주요 과정: 검색 -> 평가 -> 지식 정제 또는 웹 검색 -> 답변 생성\n",
    "\n",
    "   1. 문서 관련성 평가 (`grade_documents`):\n",
    "      - 각 문서의 관련성을 평가\n",
    "      - 기준을 통과하는 문서만을 유지\n",
    "\n",
    "   1. 지식 정제 (`refine_knowledge`):\n",
    "      - 문서를 \"지식 조각\"으로 분할하고 각각의 관련성을 평가\n",
    "      - 관련성 높은(0.5 초과) 지식 조각만 유지\n",
    "\n",
    "   1. 웹 검색 (`web_search`):\n",
    "      - 문서가 충분한 정보를 담지 못한 경우 외부 지식을 활용\n",
    "      - 웹 검색 결과를 기존 문서에 추가 \n",
    "\n",
    "   1. 답변 생성 (`generate_answer`):\n",
    "      - 정제된 지식 조각을 사용하여 답변을 생성\n",
    "      - 관련 정보가 없을 경우 적절한 메시지를 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285306dd",
   "metadata": {},
   "source": [
    "### 5-1. LLM 체인 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece863e",
   "metadata": {},
   "source": [
    "`(1) Knowledge Refiner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지식 정제를 위한 클래스\n",
    "class RefinedKnowledge(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a refined piece of knowledge extracted from a document.\n",
    "    \"\"\"\n",
    "\n",
    "    knowledge_strip: str = Field(description=\"A refined piece of knowledge extracted from a document\")\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM 모델 초기화 및 구조화된 출력 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "structured_llm_refiner = llm.with_structured_output(RefinedKnowledge)\n",
    "\n",
    "# 지식 정제를 위한 프롬프트\n",
    "system_prompt = \"\"\"\n",
    "    You are an expert in knowledge refinement. Your task is to extract key information from the given document related to the provided question and evaluate its relevance.\n",
    "\n",
    "    [Instructions]\n",
    "    1. 질문과 문서를 주의 깊게 읽습니다.\n",
    "    2. 질문에 답하는 데 관련된 문서 내의 주요 정보를 식별합니다.\n",
    "    3. 각 주요 정보에 대해:\n",
    "        a. 간결하게 추출하고 요약합니다 (정보당 1-2문장을 목표로 합니다).\n",
    "        b. 질문에 대한 관련성을 'yes' (관련 있음) 또는 'no' (관련 없음)로 평가합니다.\n",
    "    4. 각 정보를 다음 형식으로 새 줄에 제시합니다:\n",
    "        [추출된 정보] (yes/no)\n",
    "\n",
    "    [Example Output]\n",
    "    AI 시스템은 학습 데이터에 존재하는 편향을 나타낼 수 있습니다. (yes)\n",
    "    의사 결정에 AI를 사용하는 것은 프라이버시 문제를 제기합니다. (yes)\n",
    "    기계 학습 모델은 상당한 컴퓨팅 자원을 필요로 합니다. (no)\n",
    "\n",
    "    [Note]\n",
    "    Focus on extracting factual and objective information. Avoid personal opinions or speculations. Aim to provide 3-5 key pieces of information, but you may include more if the document is particularly rich in relevant content.\n",
    "    \"\"\"\n",
    "\n",
    "# 지식정제를 위한 프롬프트 템플릿 생성\n",
    "refine_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"[Document]\\n{document}\\n\\n[User question]\\n{question}\"),\n",
    "])\n",
    "\n",
    "# Knowledge Refiner 파이프라인 구성\n",
    "knowledge_refiner = refine_prompt | structured_llm_refiner\n",
    "\n",
    "# 지식 정제 실행\n",
    "retrieved_docs = search_menu.invoke(question)\n",
    "print(f\"검색된 문서 수: {len(retrieved_docs)}\")\n",
    "\n",
    "for test_chunk in retrieved_docs:\n",
    "    print(\"문서:\", test_chunk.page_content)\n",
    "\n",
    "    refined_knowledge = knowledge_refiner.invoke({\"question\": question, \"document\": test_chunk})\n",
    "    print(f\"정제된 지식: {refined_knowledge.knowledge_strip}\")\n",
    "    print(f\"정제된 지식 평가: {refined_knowledge.binary_score}\")\n",
    "    print(\"=====================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb527f",
   "metadata": {},
   "source": [
    "`(2) Retrieval Grader 수정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c79d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# 문서 관련성 평가 결과를 위한 데이터 모델 정의\n",
    "class MultiGradeDocuments(BaseModel):\n",
    "    \"\"\"Three-class score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    relevance_score: Literal[\"correct\", \"incorrect\", \"ambiguous\"] = Field(\n",
    "        description=\"Document relevance to the question: 'correct', 'incorrect', or 'ambiguous'\"\n",
    "    )\n",
    "\n",
    "# LLM 모델 초기화 및 구조화된 출력 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(MultiGradeDocuments)\n",
    "\n",
    "# 문서 관련성 평가를 위한 시스템 프롬프트 정의\n",
    "system_prompt = \"\"\"\n",
    "You are an expert evaluator tasked with assessing the relevance of retrieved documents to a user's question. Your role is crucial in enhancing the quality of information retrieval systems.\n",
    "\n",
    "[평가 기준]\n",
    "1. 키워드 관련성: 문서가 질문의 주요 단어나 유사어를 포함하는지 확인\n",
    "2. 의미적 관련성: 문서의 전반적인 주제가 질문의 의도와 일치하는지 평가\n",
    "3. 부분 관련성: 질문의 일부를 다루거나 맥락 정보를 제공하는 문서도 고려\n",
    "4. 답변 가능성: 직접적인 답이 아니더라도 답변 형성에 도움될 정보 포함 여부 평가\n",
    "\n",
    "[점수 체계]\n",
    "- 'Correct': 문서가 명확히 관련 있고, 질문에 답하는 데 필요한 정보를 포함함.\n",
    "- 'Incorrect': 문서가 명확히 무관하거나, 질문에 도움이 되지 않는 정보를 포함함.\n",
    "- 'Ambiguous': 문서의 관련성이 불분명하거나, 일부 관련 정보는 있지만 유용성이 확실하지 않음, 혹은 질문과 약간만 관련 있음.\n",
    "\n",
    "[주의사항]\n",
    "- 단순 단어 매칭이 아닌 질문의 전체 맥락을 고려하세요\n",
    "- 완벽한 답변이 아니어도 유용한 정보가 있다면 관련 있다고 판단하세요\n",
    "\n",
    "Your evaluation plays a critical role in improving the overall performance of the information retrieval system. Strive for balanced and thoughtful assessments.\n",
    "\"\"\"\n",
    "\n",
    "# 채점 프롬프트 템플릿 생성\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"Document: \\n\\n {document} \\n\\n Question: {question}\"),\n",
    "])\n",
    "\n",
    "# Retrieval Grader 파이프라인 구성\n",
    "retrieval_grader_multi = grade_prompt | structured_llm_grader\n",
    "    \n",
    "# 관련성 평가 실행\n",
    "question = \"비건 메뉴가 있나요?\"\n",
    "retrieved_docs = search_menu.invoke(question)\n",
    "print(f\"검색된 문서 수: {len(retrieved_docs)}\")\n",
    "\n",
    "for test_chunk in retrieved_docs:\n",
    "    print(\"문서:\", test_chunk.page_content)\n",
    "\n",
    "    relevance = retrieval_grader_multi.invoke({\"question\": question, \"document\": test_chunk.page_content})\n",
    "    print(f\"문서 관련성: {relevance.relevance_score}\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510ffd8",
   "metadata": {},
   "source": [
    "### 5-2. LangGraph로 그래프 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107236a3",
   "metadata": {},
   "source": [
    "`(1) 그래프 State 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph MessagesState라는 미리 만들어진 상태를 사용\n",
    "from langgraph.graph import MessagesState\n",
    "from typing import List, Tuple\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class CorrectiveRagState(MessagesState):\n",
    "    \"\"\"\n",
    "    메시지 기반 그래프의 상태를 나타내는 클래스\n",
    "    \n",
    "    Attributes:\n",
    "        messages: 대화 히스토리 (기본 제공, 설정 불필요)\n",
    "        question: 사용자의 질문\n",
    "        generation: AI 모델의 답변\n",
    "        retrieved_documents: 검색된 문서 리스트 (문서, 점수)\n",
    "        knowledge_strips: 지식 보강한 결과 리스트 (문서, 점수)\n",
    "        num_generations: 생성 횟수 (무한 루프 방지에 활용)\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    retrieved_documents: List[Tuple[Document, str]] \n",
    "    knowledge_strips: List[Tuple[Document, str]] \n",
    "    num_generations: int "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673cf479",
   "metadata": {},
   "source": [
    "`(2) Node 구성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc40be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def retrieve_crag(state: CorrectiveRagState):\n",
    "    \"\"\"문서를 검색하는 함수\"\"\"\n",
    "    print(\"--- 문서 검색 ---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 문서 검색 로직\n",
    "    retrieved_documents = search_menu.invoke(question)\n",
    "    retrieved_documents = [(doc, \"ambiguous\") for doc in retrieved_documents]\n",
    "\n",
    "    return {\"retrieved_documents\": retrieved_documents}\n",
    "\n",
    "def grade_documents_crag(state: CorrectiveRagState):\n",
    "    \"\"\"검색된 문서의 관련성을 평가하는 함수\"\"\"\n",
    "    print(\"--- 문서 관련성 평가 ---\")\n",
    "    question = state[\"question\"]\n",
    "    retrieved_documents = state.get(\"retrieved_documents\", [])\n",
    "    knowledge_strips = state.get(\"knowledge_strips\", [])\n",
    "    \n",
    "    scored_docs = []\n",
    "    for doc, _ in retrieved_documents:\n",
    "        score = retrieval_grader_multi.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "        grade = score.relevance_score.lower()\n",
    "        if grade == \"correct\":\n",
    "            print(\"---문서 관련성: 있음---\")\n",
    "            scored_docs.append((doc, \"correct\"))\n",
    "            knowledge_strips.append((doc, \"correct\"))\n",
    "        elif grade == \"incorrect\":\n",
    "            print(\"---문서 관련성: 없음---\")\n",
    "            scored_docs.append((doc, \"incorrect\"))\n",
    "        else:\n",
    "            print(\"---문서 관련성: 모호함---\")\n",
    "            scored_docs.append((doc, \"ambiguous\"))\n",
    "\n",
    "    return {\"retrieved_documents\": scored_docs, \"knowledge_strips\": knowledge_strips}\n",
    "\n",
    "def refine_knowledge_crag(state: CorrectiveRagState):\n",
    "    \"\"\"지식을 정제하는 함수\"\"\"\n",
    "    print(\"--- 지식 정제 ---\")\n",
    "    question = state[\"question\"]\n",
    "    knowledge_strips = state.get(\"knowledge_strips\", [])\n",
    "    \n",
    "    refined_docs = []\n",
    "    for doc, score in knowledge_strips:\n",
    "        if score == \"incorrect\":\n",
    "            continue\n",
    "\n",
    "        refined_knowledge = knowledge_refiner.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "        knowledge = refined_knowledge.knowledge_strip\n",
    "        grade = refined_knowledge.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---정제된 지식: 추가---\")\n",
    "            refined_docs.append((Document(page_content=knowledge), \"correct\"))\n",
    "    \n",
    "    return {\"knowledge_strips\": refined_docs}\n",
    "\n",
    "def web_search_crag(state: CorrectiveRagState):\n",
    "    \"\"\"웹 검색을 수행하는 함수\"\"\"\n",
    "    print(\"--- 웹 검색 ---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    search_results = search_web.invoke(question)\n",
    "\n",
    "    scored_docs = []\n",
    "    for result in search_results:\n",
    "        score = retrieval_grader_multi.invoke({\"question\": question, \"document\": result})\n",
    "        grade = score.relevance_score.lower()\n",
    "        if grade == \"correct\":\n",
    "            print(\"---웹 검색 문서 관련성: 있음---\")\n",
    "            scored_docs.append((result, \"correct\"))\n",
    "        else:\n",
    "            print(\"---웹 검색 문서 관련성: 없음---\")\n",
    "\n",
    "    return {\"knowledge_strips\": scored_docs}\n",
    "\n",
    "def generate_crag(state: CorrectiveRagState):\n",
    "    \"\"\"답변을 생성하는 함수\"\"\"\n",
    "    print(\"--- 답변 생성 ---\")\n",
    "    question = state[\"question\"]\n",
    "    knowledge_strips = state.get(\"knowledge_strips\", [])\n",
    "    \n",
    "    doc_texts = [doc for doc, _ in knowledge_strips]\n",
    "    generation = generator_rag_answer(question, docs=doc_texts)\n",
    "\n",
    "    return {\n",
    "        \"generation\": generation, \n",
    "        \"messages\": [AIMessage(content=generation)]            # 기존 메시지에 추가 \n",
    "    }\n",
    "\n",
    "def transform_query_crag(state: CorrectiveRagState):\n",
    "    \"\"\"질문을 개선하는 함수\"\"\"\n",
    "    print(\"--- 질문 개선 ---\")\n",
    "    question = state[\"question\"]\n",
    "    num_generations = state.get(\"num_generations\", 0)\n",
    "    num_generations += 1\n",
    "    \n",
    "    rewritten_question = rewrite_question(question)\n",
    "    \n",
    "    return {\n",
    "        \"question\": rewritten_question,\n",
    "        \"num_generations\": num_generations,\n",
    "        \"messages\": [HumanMessage(content=rewritten_question)]  # 기존 메시지에 추가 \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57943fd4",
   "metadata": {},
   "source": [
    "`(3) Edge 구성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate_crag(state: CorrectiveRagState):\n",
    "    \"\"\"답변 생성 여부를 결정하는 함수\"\"\"\n",
    "    print(\"--- 평가된 문서 분석 ---\")\n",
    "    knowledge_strips = state.get(\"knowledge_strips\", None)\n",
    "    \n",
    "    if not knowledge_strips:\n",
    "        print(\"--- 결정: 모든 문서가 질문과 관련이 없음, 질문 개선 필요 (-> transform_query)---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"--- 결정: 답변 생성 (-> generate)---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae76a5e",
   "metadata": {},
   "source": [
    "`(4) 그래프 연결`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ec776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# 워크플로우 그래프 초기화\n",
    "builder = StateGraph(CorrectiveRagState)\n",
    "\n",
    "# 노드 정의\n",
    "builder.add_node(\"retrieve\", retrieve_crag)                  # 문서 검색 \n",
    "builder.add_node(\"grade_documents\", grade_documents_crag)    # 문서 평가\n",
    "builder.add_node(\"refine_knowledge\", refine_knowledge_crag)  # 지식 정제\n",
    "builder.add_node(\"web_search\", web_search_crag)              # 웹 검색\n",
    "builder.add_node(\"generate\", generate_crag)                  # 답변 생성\n",
    "builder.add_node(\"transform_query\", transform_query_crag)    # 질문 개선\n",
    "\n",
    "\n",
    "# 경로 정의\n",
    "builder.add_edge(START, \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"grade_documents\")\n",
    "builder.add_edge(\"grade_documents\", \"refine_knowledge\")\n",
    "\n",
    "# 조건부 엣지 추가: 문서 평가 후 결정\n",
    "builder.add_conditional_edges(\n",
    "    \"refine_knowledge\",\n",
    "    decide_to_generate_crag,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 추가 경로 \n",
    "builder.add_edge(\"transform_query\", \"web_search\")\n",
    "builder.add_edge(\"web_search\", \"generate\")\n",
    "builder.add_edge(\"generate\", END)\n",
    "\n",
    "\n",
    "# 그래프 컴파일\n",
    "corrective_rag = builder.compile()\n",
    "\n",
    "# 그래프 시각화\n",
    "display(Image(corrective_rag.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0192fa7",
   "metadata": {},
   "source": [
    "`(5) 그래프 실행`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61252c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"스테이크 메뉴의 가격은 얼마인가요?\"}\n",
    "\n",
    "for output in corrective_rag.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # 노드 출력\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        pprint(f\"Value: {value}\", indent=2, width=80, depth=None)\n",
    "    print(\"\\n----------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb891b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답변\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9dbf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"스테이크에 어울리는 와인을 추천해주세요.\"}\n",
    "\n",
    "for output in corrective_rag.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # 노드 출력\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        pprint(f\"Value: {value}\", indent=2, width=80, depth=None)\n",
    "    print(\"\\n----------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c3792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답변\n",
    "print(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-0lCeHk3W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
